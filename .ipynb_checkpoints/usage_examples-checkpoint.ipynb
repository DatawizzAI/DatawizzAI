{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to DataWizz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the functionalities of our Python package, there are a few preliminary steps you'll need to follow to set up your environment. This guide will walk you through importing required packages, loading environment variables, and initializing the language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all the necessary packages that our application relies on. This includes utilities for loading environment variables, interfacing with OpenAI's language models, defining data structures, and consuming data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:07:30.110564Z",
     "start_time": "2024-05-05T12:07:30.107453Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from src.DataDefiner import *\n",
    "from src.DataAugmentor import DataAugmentor\n",
    "from src.utils.utils import parse_output, try_parse_json, create_json_sample_from_csv, compose_query_message"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure these packages are installed in your Python environment. You may need to install some of them using pip if they are not already present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment variables are used to store configurations and sensitive information securely. Use dotenv to load these variables from a .env file in your project directory."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:07:32.944011Z",
     "start_time": "2024-05-05T12:07:32.935322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#install if you don't have this package already\n",
    "#!pip install python-dotenv\n",
    "\n",
    "#import relevant functions\n",
    "from dotenv import load_dotenv, find_dotenv"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T08:42:52.039555Z",
     "start_time": "2024-05-02T08:42:52.034290Z"
    }
   },
   "source": [
    "# Load environment variables\n",
    "_ = load_dotenv(find_dotenv())"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, initialize the language model from OpenAI. In this example, we're using the gpt-3.5-turbo model with a specific temperature setting. The ChatOpenAI class is a part of the langchain_openai package and facilitates interaction with OpenAI's API."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:42:56.444598Z",
     "start_time": "2024-05-02T08:42:55.893848Z"
    }
   },
   "source": [
    "# please make sure OPENAI_API_KEY is loaded to your environment variables\n",
    "# Initialize language model\n",
    "llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature parameter controls the randomness of the model's responses, with higher values leading to more varied outputs. Adjust this setting according to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing these steps, your environment is ready, and you can proceed with the specific functionalities of the package, such as defining data structures, generating datasets, and more."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Init HuggingFace Endpoint - OpenAI alternative "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:26:07.466201Z",
     "start_time": "2024-05-02T08:26:07.462181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ! pip install huggingface_hub\n",
    "# \n",
    "# # please make sure HUGGINGFACEHUB_API_TOKEN is loaded to your environment variables\n",
    "# from langchain_community.llms import HuggingFaceEndpoint\n",
    "# \n",
    "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# \n",
    "# llm = HuggingFaceEndpoint(repo_id=repo_id, max_new_tokens=2048)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines the various pipeline types available within the package, each designed to transform inputs into structured or unstructured data sets for different applications such as machine learning, databases, or flexible data formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DescriptionToMLDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal for users who want to generate machine learning datasets based on descriptive inputs. This pipeline interprets natural language descriptions of data requirements and structures, then produces datasets optimized for training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:05:24.325833Z",
     "start_time": "2024-05-05T12:05:24.264232Z"
    }
   },
   "source": [
    "#Initialize a DataStructureDefiner\n",
    "pipeline_name = get_pipeline_name('DescriptionToMLDataset')\n",
    "DataDefinerObj = DataDefiner(llm, pipeline_name = pipeline_name)\n",
    "\n",
    "#Define the required data structure and view the result structure\n",
    "# Example non-expert description\n",
    "dataStructureDescription = \"Generate a machine learning dataset for predicting suspicious AML transactions based on features like customer characteristics, transaction characteristics, location, and transactions history characteristics.\"\n",
    "\n",
    "# Generate professional specifications\n",
    "task_specifications = TaskSpecificationAugmentor.generate_specifications_from_description(llm, description = dataStructureDescription)\n",
    "\n",
    "# Print the generated specifications\n",
    "print(\"Generated Specifications:\")\n",
    "print(task_specifications)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Initialize a DataStructureDefiner\u001B[39;00m\n\u001B[1;32m      2\u001B[0m pipeline_name \u001B[38;5;241m=\u001B[39m get_pipeline_name(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDescriptionToMLDataset\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m DataDefinerObj \u001B[38;5;241m=\u001B[39m DataDefiner(\u001B[43mllm\u001B[49m, pipeline_name \u001B[38;5;241m=\u001B[39m pipeline_name)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#Define the required data structure and view the result structure\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Example non-expert description\u001B[39;00m\n\u001B[1;32m      7\u001B[0m dataStructureDescription \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerate a machine learning dataset for predicting suspicious AML transactions based on features like customer characteristics, transaction characteristics, location, and transactions history characteristics.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'llm' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the textual description, aided with the expert specification, into a sample of the needed data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:26:37.116082Z",
     "start_time": "2024-05-02T08:26:15.470821Z"
    }
   },
   "source": [
    "dataStructureSample = DataDefinerObj.define_schema_from_description(description = dataStructureDescription, task_specifications=task_specifications)\n",
    "print(parse_output(dataStructureSample))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate an initial synthetic data sample for the described task (according to the user guidance if provided, or your knowledge otherwise).Provide a sample with 10 number of items; items can be records in a table,records that can potentially be joined in a DB schema, or the equivalent tuples in a JSON format in unstructured text.The task as described by the user: Generate a machine learning dataset for predicting suspicious AML transactions based on features like customer characteristics, transaction characteristics, location, and transactions history characteristics.;The guidance given by an expert: 1. \n",
      "- Customer ID: Unique identifier\n",
      "- Age: Numerical\n",
      "- Gender: Categorical\n",
      "- Occupation: Categorical\n",
      "- Income: Numerical\n",
      "- Address: Free text\n",
      "- Transaction ID: Unique identifier\n",
      "- Date/time: Datetime\n",
      "- Transaction amount: Numerical\n",
      "- Location: Categorical\n",
      "- Transaction history: Free text\n",
      "\n",
      "2.\n",
      "- Age: Distribution skewed right, mean 35, std 10, min 18, max 60\n",
      "- Gender: Categories (Male, Female, Other) with probabilities (0.6, 0.3, 0.1)\n",
      "- Occupation: Categories (Manager, Accountant, Teacher) with probabilities (0.4, 0.3, 0.3)\n",
      "- Income: Distribution skewed right, mean $50,000, std $20,000, min $20,000, max $100,000\n",
      "- Address: Mean text length 50 characters, std 10 characters\n",
      "- Transaction amount: Distribution skewed right, mean $500, std $200, min $100, max $1000\n",
      "- Location: Categories (Urban, Suburban, Rural) with probabilities (0.5, 0.3, 0.2)\n",
      "- Transaction history: Mean text length 100 characters, std 20 characters\n",
      "\n",
      "3. Important correlations to maintain include the relationship between customer characteristics and transaction characteristics, as well as the influence of location on transaction behavior. Dependencies may include income level affecting transaction amount and transaction history influencing suspicious activity.\n",
      "\n",
      "4. Fields better generated by a LLM instead of simple code would include predicting suspicious transactions based on a combination of customer characteristics, transaction characteristics, and historical data.\n",
      "\n",
      "5. This table has a sequential (transactional) nature as it includes information about transactions over time, indicating a flow of data related to customer behavior and financial activities.;Format instructions: Generate Sample synthetic records for tabular data (preferably a single flattened table) that can be used for training a well-performing ML model for the task described in the data description.                Please include any relevant attribute you can think of, as we do not want to miss any feature that the user may find useful.                Format the output as JSON with the dataset name as key.Please make sure you output a valid JSON format, and don't cut it in the middle.Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\n",
      "Generated Output:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'AML_Transactions_Dataset':   Customer ID  Age  Gender  Occupation  Income            Address  \\\n",
      "0       C1001   32    Male     Manager   54000    123 Main Street   \n",
      "1       C1002   42  Female     Teacher   72000     456 Elm Street   \n",
      "2       C1003   27   Other  Accountant   35000     789 Oak Street   \n",
      "3       C1004   50    Male     Manager   90000    101 Pine Street   \n",
      "4       C1005   22  Female  Accountant   25000   202 Maple Street   \n",
      "5       C1006   38    Male     Teacher   60000  303 Walnut Street   \n",
      "6       C1007   45  Female     Manager   80000   404 Cedar Street   \n",
      "7       C1008   55   Other     Teacher   72000   505 Birch Street   \n",
      "8       C1009   30    Male  Accountant   40000     606 Ash Street   \n",
      "9       C1010   48  Female     Manager   75000  707 Spruce Street   \n",
      "\n",
      "  Transaction ID            Date/time  Transaction amount  Location  \\\n",
      "0           T001  2022-01-15 08:30:00                 480     Urban   \n",
      "1           T002  2022-02-10 14:45:00                 620  Suburban   \n",
      "2           T003  2022-03-05 10:20:00                 300     Urban   \n",
      "3           T004  2022-04-20 16:00:00                 800  Suburban   \n",
      "4           T005  2022-05-12 09:45:00                 150     Rural   \n",
      "5           T006  2022-06-25 11:30:00                 550  Suburban   \n",
      "6           T007  2022-07-18 13:15:00                 700     Urban   \n",
      "7           T008  2022-08-30 15:00:00                 840     Urban   \n",
      "8           T009  2022-09-14 18:45:00                 400  Suburban   \n",
      "9           T010  2022-10-05 17:30:00                 670     Urban   \n",
      "\n",
      "                    Transaction history  \n",
      "0              Regular monthly expenses  \n",
      "1                 Recent large purchase  \n",
      "2           Frequent small transactions  \n",
      "3           Limited transaction history  \n",
      "4                   First-time customer  \n",
      "5          Consistent moderate spending  \n",
      "6  Significant increase in transactions  \n",
      "7              Varied transaction types  \n",
      "8            Occasional large purchases  \n",
      "9          Unusual transaction patterns  }\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:26:37.119621Z",
     "start_time": "2024-05-02T08:26:37.117087Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DescriptionToDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this pipeline when you need to create a relational database schema and populate it with data, all derived from a natural language description. It’s perfect for quickly prototyping database designs or for users less familiar with SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T12:04:04.985114Z",
     "start_time": "2024-05-05T12:04:04.882357Z"
    }
   },
   "source": [
    "#Initialize a DataStructureDefiner\n",
    "pipeline_name = get_pipeline_name('DescriptionToDB')\n",
    "DataDefinerObj = DataDefiner(llm, pipeline_name = pipeline_name)\n",
    "\n",
    "#Define the required data structure and view the result structure\n",
    "dataStructureDescription = \"Create a database schema for a bookstore that includes tables for books, authors, and sales transactions.\"\n",
    "dataStructureSample = DataDefinerObj.define_schema_from_description(dataStructureDescription)\n",
    "print(parse_output(dataStructureSample))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_pipeline_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Initialize a DataStructureDefiner\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m pipeline_name \u001B[38;5;241m=\u001B[39m \u001B[43mget_pipeline_name\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDescriptionToDB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m DataDefinerObj \u001B[38;5;241m=\u001B[39m DataDefiner(llm, pipeline_name \u001B[38;5;241m=\u001B[39m pipeline_name)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#Define the required data structure and view the result structure\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'get_pipeline_name' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQLToTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline is designed for users who prefer to define their data structure using SQL syntax. It takes SQL table creation statements and query specifications, then generates a corresponding structured, relational database schema."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:26:54.954597Z",
     "start_time": "2024-05-02T08:26:44.264499Z"
    }
   },
   "source": [
    "#Initialize a DataStructureDefiner\n",
    "pipeline_name = get_pipeline_name('SQLToTabular')\n",
    "DataDefinerObj = DataDefiner(llm, pipeline_name = pipeline_name)\n",
    "\n",
    "#Define the required data structure and view the result structure\n",
    "dataStructureDescription = \"-- Create a 'Customers' table with customer information\\\n",
    "CREATE TABLE Customers (\\\n",
    "    CustomerID INT PRIMARY KEY,\\\n",
    "    FirstName VARCHAR(50),\\\n",
    "    LastName VARCHAR(50),\\\n",
    "    Email VARCHAR(100),\\\n",
    "    JoinDate DATE\\\n",
    ");\\\n",
    "\\\n",
    "-- Create an 'Orders' table with order details\\\n",
    "CREATE TABLE Orders (\\\n",
    "    OrderID INT PRIMARY KEY,\\\n",
    "    CustomerID INT,\\\n",
    "    OrderDate DATE,\\\n",
    "    ProductCategory VARCHAR(50),\\\n",
    "    ProductName VARCHAR(100),\\\n",
    "    Units INT,\\\n",
    "    TotalAmount DECIMAL(10, 2),\\\n",
    "    FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID)\\\n",
    ");\"\n",
    "dataStructureSample = DataDefinerObj.define_schema_from_description(dataStructureDescription)\n",
    "print(parse_output(dataStructureSample))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate an initial synthetic data sample for the described task (according to the user guidance if provided, or your knowledge otherwise).Provide a sample with 10 number of items; items can be records in a table,records that can potentially be joined in a DB schema, or the equivalent tuples in a JSON format in unstructured text.The task as described by the user: -- Create a 'Customers' table with customer informationCREATE TABLE Customers (    CustomerID INT PRIMARY KEY,    FirstName VARCHAR(50),    LastName VARCHAR(50),    Email VARCHAR(100),    JoinDate DATE);-- Create an 'Orders' table with order detailsCREATE TABLE Orders (    OrderID INT PRIMARY KEY,    CustomerID INT,    OrderDate DATE,    ProductCategory VARCHAR(50),    ProductName VARCHAR(100),    Units INT,    TotalAmount DECIMAL(10, 2),    FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID));;The guidance given by an expert: None;Format instructions: Generate Sample synthetic data that fit to the structure given by the SQL command that appears in the data description.                Please follow any relevant distributions for the stated fields, as we want this data to be as valid and useful as possible for development and testing.                Format the output as JSON with each table name as key.Please make sure you output a valid JSON format, and don't cut it in the middle.Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\n",
      "Generated Output:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'Customers':    CustomerID FirstName  LastName                     Email    JoinDate\n",
      "0           1      John       Doe         johndoe@email.com  2021-01-15\n",
      "1           2      Jane     Smith       janesmith@email.com  2021-02-20\n",
      "2           3   Michael   Johnson  michaeljohnson@email.com  2020-12-10\n",
      "3           4     Sarah  Williams   sarahwilliams@email.com  2020-11-05\n",
      "4           5     David     Brown      davidbrown@email.com  2021-03-25, 'Orders':    OrderID  CustomerID   OrderDate ProductCategory ProductName  Units  \\\n",
      "0        1           1  2021-04-05     Electronics      Laptop      1   \n",
      "1        2           2  2021-03-15        Clothing       Dress      2   \n",
      "2        3           3  2021-02-28      Home goods    Curtains      4   \n",
      "3        4           4  2021-01-10           Books       Novel      1   \n",
      "4        5           5  2021-05-20  Sporting Goods    Yoga Mat      3   \n",
      "\n",
      "   TotalAmount  \n",
      "0      1200.50  \n",
      "1       150.75  \n",
      "2        75.20  \n",
      "3        20.00  \n",
      "4        45.60  }\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DescriptionToUnstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose this pipeline for scenarios where structured data formats are not required. It enables the creation of datasets with flexible formats, such as text blobs, documents, or other forms of unstructured data, based on descriptive inputs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:27:14.737380Z",
     "start_time": "2024-05-02T08:26:54.955638Z"
    }
   },
   "source": [
    "#Initialize a DataStructureDefiner\n",
    "pipeline_name = get_pipeline_name('DescriptionToUnstructured')\n",
    "DataDefinerObj = DataDefiner(llm, pipeline_name = pipeline_name)\n",
    "\n",
    "#Define the required data structure and view the result structure\n",
    "dataStructureDescription = \"Generate a collection of news articles about recent technological advancements in renewable energy.\"\n",
    "dataStructureSample = DataDefinerObj.define_schema_from_description(dataStructureDescription)\n",
    "print(parse_output(dataStructureSample))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate an initial synthetic data sample for the described task (according to the user guidance if provided, or your knowledge otherwise).Provide a sample with 10 number of items; items can be records in a table,records that can potentially be joined in a DB schema, or the equivalent tuples in a JSON format in unstructured text.The task as described by the user: Generate a collection of news articles about recent technological advancements in renewable energy.;The guidance given by an expert: None;Format instructions: Generate Sample data, potentially unstructured, that fit to the data description.                Format the output as JSONL with the dataset name as key and each line contains a single sampled text in the appropriate format.Please make sure you output a valid JSON format, and don't cut it in the middle.Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\n",
      "Generated Output:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'news_articles_technological_advancements_renewable_energy':                                                title        date  \\\n",
      "0  New Solar Panel Design Increases Efficiency by...  2021-05-15   \n",
      "1  Wind Turbines with AI Optimization Boost Energ...  2021-06-02   \n",
      "2  Breakthrough in Energy Storage Technology Enab...  2021-07-10   \n",
      "3  Hydrogen Fuel Cells Poised to Disrupt Transpor...  2021-08-20   \n",
      "4  Innovative Wave Energy Converter Harvests Ener...  2021-09-05   \n",
      "5  Advancements in Perovskite Solar Cells Make th...  2021-10-12   \n",
      "6  Smart Grid Technology Enables Dynamic Energy M...  2021-11-25   \n",
      "7  Biodegradable Solar Panels Offer Sustainable S...  2021-12-05   \n",
      "8  Blockchain Technology Streamlines Energy Tradi...  2022-01-10   \n",
      "9  New Biofuels Show Promise as Clean Alternative...  2022-02-20   \n",
      "\n",
      "                            source  \\\n",
      "0                   TechNews Today   \n",
      "1         Renewable Energy Journal   \n",
      "2               CleanTech Magazine   \n",
      "3             Future Energy Trends   \n",
      "4   Marine Renewable Energy Report   \n",
      "5         Solar Energy Innovations   \n",
      "6          Energy Efficiency Today   \n",
      "7         Green Technology Gazette   \n",
      "8  Renewable Energy Finance Weekly   \n",
      "9       Bioenergy Research Journal   \n",
      "\n",
      "                                             content  \n",
      "0  A team of researchers has developed a new sola...  \n",
      "1  Wind turbines equipped with artificial intelli...  \n",
      "2  A breakthrough in energy storage technology ha...  \n",
      "3  Hydrogen fuel cells are poised to disrupt the ...  \n",
      "4  An innovative wave energy converter has been d...  \n",
      "5  Recent advancements in perovskite solar cells ...  \n",
      "6  Smart grid technology has enabled dynamic ener...  \n",
      "7  Biodegradable solar panels offer a sustainable...  \n",
      "8  Blockchain technology has been used to streaml...  \n",
      "9  New biofuels are showing promise as a clean al...  }\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. APINameToData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize this pipeline when you have an API specification (e.g., OpenAPI/Swagger) and want to generate data that conforms to the defined endpoints and data models. It’s ideal for testing API endpoints or generating mock data for API development."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:27:34.709525Z",
     "start_time": "2024-05-02T08:27:14.738427Z"
    }
   },
   "source": [
    "#Initialize a DataStructureDefiner\n",
    "pipeline_name = get_pipeline_name('APISpecificationToData')\n",
    "DataDefinerObj = DataDefiner(llm, pipeline_name = pipeline_name)\n",
    "\n",
    "#Define the required data structure and view the result structure\n",
    "dataStructureDescription = \"Generate mock data conforming to Twitter's API. The data should include user profiles and tweets, along with metadata for engagements (likes, retweets, replies).\"\n",
    "task_specifications = TaskSpecificationAugmentor.generate_specifications_from_description(llm, description = dataStructureDescription)\n",
    "\n",
    "dataStructureSample = DataDefinerObj.define_schema_from_description(dataStructureDescription, task_specifications=task_specifications)\n",
    "print(parse_output(dataStructureSample))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are an analyst whose job is to conduct a deep research for a given task, and define the needed data to collect, making sure you don't miss any relevant detail, and gain a deep understanding of the data characteristics.1.  Generate the names of the columns relevant for the description of the user. Use indicative names. Assign each column its type - numerical, categorical, datetime, free text, unique identifier. 2. Revisit each column and complete these details:For numeric columns - describe its distribution, mean and std, min and max values. for numbers and datetimes define the needed format, for categorical columns detail a complete set of categories and its probabilities. For free text columns - specify the mean and std of the text length, For unique identifier columns - specify the format and regEX to follow. For Datetime columns - specify min and max values, as well as the time intervals mean and std values. 3. Important correlations to maintain or other constraints and dependencies. 4. Indicate what fields would be better to be generated by a LLM instead of by a simple code. 5. Indicate if this table is has sequential (transactional) nature. \n",
      "The task as described by the user: Generate mock data conforming to Twitter's API. The data should include user profiles and tweets, along with metadata for engagements (likes, retweets, replies).;Please make sure you output a valid textual description, just guidance, no intro and summary are needed, and don't cut it in the middle.\n",
      "Generated Extracted Logic:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate an initial synthetic data sample for the described task (according to the user guidance if provided, or your knowledge otherwise).Provide a sample with 10 number of items; items can be records in a table,records that can potentially be joined in a DB schema, or the equivalent tuples in a JSON format in unstructured text.The task as described by the user: Generate mock data conforming to Twitter's API. The data should include user profiles and tweets, along with metadata for engagements (likes, retweets, replies).;The guidance given by an expert: 1. Relevant Columns:\n",
      "- User ID (unique identifier)\n",
      "- Username (free text)\n",
      "- Bio (free text)\n",
      "- Location (categorical)\n",
      "- Followers Count (numerical)\n",
      "- Following Count (numerical)\n",
      "- Tweet Count (numerical)\n",
      "- Account Creation Date (datetime)\n",
      "- Tweet ID (unique identifier)\n",
      "- Tweet Text (free text)\n",
      "- Tweet Date (datetime)\n",
      "- Likes Count (numerical)\n",
      "- Retweets Count (numerical)\n",
      "- Replies Count (numerical)\n",
      "\n",
      "2. Column Details:\n",
      "- User ID: format - alphanumeric, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Username: mean text length - 7, std text length - 2\n",
      "- Location: categories - [USA, UK, Canada, Australia, Other], probabilities - [0.3, 0.2, 0.1, 0.1, 0.3]\n",
      "- Followers Count: distribution - positively skewed, mean - 1000, std - 500, min - 100, max - 5000\n",
      "- Following Count: distribution - positively skewed, mean - 500, std - 200, min - 50, max - 2000\n",
      "- Tweet Count: distribution - normally distributed, mean - 2000, std - 1000, min - 500, max - 5000\n",
      "- Account Creation Date: format - dd/mm/yyyy, min date - 01/01/2010, max date - 31/12/2022, time intervals mean - 2 years, std - 1 year\n",
      "- Tweet ID: format - alphanumeric, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Tweet Text: mean text length - 100, std text length - 20\n",
      "- Tweet Date: format - dd/mm/yyyy hh:mm:ss, min date - 01/01/2010, max date - 31/12/2022, time intervals mean - 1 day, std - 6 hours\n",
      "- Likes Count: distribution - positively skewed, mean - 100, std - 50, min - 10, max - 500\n",
      "- Retweets Count: distribution - positively skewed, mean - 50, std - 20, min - 5, max - 200\n",
      "- Replies Count: distribution - normally distributed, mean - 20, std - 10, min - 1, max - 100\n",
      "\n",
      "3. Constraints and Dependencies: \n",
      "- User ID is unique and should be maintained as the primary key.\n",
      "- Tweet ID is unique and should be maintained as the primary key.\n",
      "- Engagements counts (likes, retweets, replies) should be dependent on the respective tweet.\n",
      "\n",
      "4. Fields for LLM:\n",
      "- Generation of tweet text content could benefit from a language model to ensure coherence and relevance.\n",
      "\n",
      "5. Sequential Nature:\n",
      "- This table has a sequential nature based on the chronological order of account creation dates and tweet dates.;Format instructions: Check the input and output of the API that is mentioned in the user desription, and generate synthetic API calls (with valid input and output) that fit to the required structure of this API.                Format your output as JSONL, with the actual API name as key and each line containing the input and output of a synthetic API call.Please make sure you output a valid JSON format, and don't cut it in the middle.Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\n",
      "Generated Output:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'twitter_api':   input.user_id input.username                         input.bio  \\\n",
      "0     u2387hj10        user123        Lorem ipsum dolor sit amet   \n",
      "1     u3985klmn       abc_user       Consectetur adipiscing elit   \n",
      "2     u4567opqr      test_user  Sed do eiusmod tempor incididunt   \n",
      "\n",
      "  input.location  input.followers_count  input.following_count  \\\n",
      "0            USA                   1200                    600   \n",
      "1             UK                   1500                    700   \n",
      "2         Canada                   1300                    550   \n",
      "\n",
      "   input.tweet_count input.account_creation_date input.tweet_id  \\\n",
      "0               2300                  15/06/2015       t4390dfg   \n",
      "1               2600                  25/03/2012       t5678qwe   \n",
      "2               2100                  08/12/2018       t6789zxc   \n",
      "\n",
      "                                    input.tweet_text     input.tweet_date  \\\n",
      "0                        This is a sample tweet text  20/08/2015 14:30:00   \n",
      "1                          Another sample tweet text  10/10/2012 09:45:00   \n",
      "2  Lorem ipsum dolor sit amet, consectetur adipis...  05/05/2019 17:20:00   \n",
      "\n",
      "   input.likes_count  input.retweets_count  input.replies_count output.status  \n",
      "0                110                    55                   25       success  \n",
      "1                 90                    45                   20       success  \n",
      "2                120                    60                   30       success  }\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quering the Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up your data structure, the next step is to generate the data by refining content according to your specific needs and generating a data preview. This section guides you through the process of specifying content refinements such as region and language, initializing the data generator, and generating the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a DataAugmentor object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initialize the DataAugmentor object with your chosen language model (llm) and the predefined data structure (dataStructureSample), and with the specification for the defined data structure:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:27:34.714234Z",
     "start_time": "2024-05-02T08:27:34.710534Z"
    }
   },
   "source": [
    "# Content refinements\n",
    "region = 'France'\n",
    "language = 'French'\n",
    "query = \"\" "
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:27:48.513025Z",
     "start_time": "2024-05-02T08:27:34.718244Z"
    }
   },
   "source": [
    "# Revisiting expert specifications to adjust it to the user query\n",
    "full_query = compose_query_message(query = query, region=region, language=language)\n",
    "updated_task_specifications = TaskSpecificationAugmentor.refine_specifications_by_description(llm=llm,\n",
    "                                                                                           description=full_query, previous_task_specification = task_specifications)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are an analyst whose job is to conduct a deep research for a given task, and define the needed data to collect, making sure you don't miss any relevant detail, and gain a deep understanding of the data characteristics.You already gave instructions for the needed data (see Previous Task Specifications), but now the user asks a content refinement (see User Query). Please revisit the columns distributions, and descriptive statistics and update those that have changed due to the user query. \n",
      "The User Query: Data description:  ; The required region: France ; All texts should be translated to French language.;\n",
      "Your Previous Task Specifications: 1. Relevant Columns:\n",
      "- User ID (unique identifier)\n",
      "- Username (free text)\n",
      "- Bio (free text)\n",
      "- Location (categorical)\n",
      "- Followers Count (numerical)\n",
      "- Following Count (numerical)\n",
      "- Tweet Count (numerical)\n",
      "- Account Creation Date (datetime)\n",
      "- Tweet ID (unique identifier)\n",
      "- Tweet Text (free text)\n",
      "- Tweet Date (datetime)\n",
      "- Likes Count (numerical)\n",
      "- Retweets Count (numerical)\n",
      "- Replies Count (numerical)\n",
      "\n",
      "2. Column Details:\n",
      "- User ID: format - alphanumeric, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Username: mean text length - 7, std text length - 2\n",
      "- Location: categories - [USA, UK, Canada, Australia, Other], probabilities - [0.3, 0.2, 0.1, 0.1, 0.3]\n",
      "- Followers Count: distribution - positively skewed, mean - 1000, std - 500, min - 100, max - 5000\n",
      "- Following Count: distribution - positively skewed, mean - 500, std - 200, min - 50, max - 2000\n",
      "- Tweet Count: distribution - normally distributed, mean - 2000, std - 1000, min - 500, max - 5000\n",
      "- Account Creation Date: format - dd/mm/yyyy, min date - 01/01/2010, max date - 31/12/2022, time intervals mean - 2 years, std - 1 year\n",
      "- Tweet ID: format - alphanumeric, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Tweet Text: mean text length - 100, std text length - 20\n",
      "- Tweet Date: format - dd/mm/yyyy hh:mm:ss, min date - 01/01/2010, max date - 31/12/2022, time intervals mean - 1 day, std - 6 hours\n",
      "- Likes Count: distribution - positively skewed, mean - 100, std - 50, min - 10, max - 500\n",
      "- Retweets Count: distribution - positively skewed, mean - 50, std - 20, min - 5, max - 200\n",
      "- Replies Count: distribution - normally distributed, mean - 20, std - 10, min - 1, max - 100\n",
      "\n",
      "3. Constraints and Dependencies: \n",
      "- User ID is unique and should be maintained as the primary key.\n",
      "- Tweet ID is unique and should be maintained as the primary key.\n",
      "- Engagements counts (likes, retweets, replies) should be dependent on the respective tweet.\n",
      "\n",
      "4. Fields for LLM:\n",
      "- Generation of tweet text content could benefit from a language model to ensure coherence and relevance.\n",
      "\n",
      "5. Sequential Nature:\n",
      "- This table has a sequential nature based on the chronological order of account creation dates and tweet dates.;Please make sure you output a valid textual description, just guidance, no intro and summary are needed, and don't cut it in the middle.\n",
      "Generated Extracted Logic:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:27:48.523056Z",
     "start_time": "2024-05-02T08:27:48.518055Z"
    }
   },
   "source": [
    "# Initialize DataConsumer\n",
    "DataAugmentorObj = DataAugmentor(llm=llm, structure=dataStructureSample)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a preview of your dataset to ensure it meets your requirements. You can optionally provide a query to further guide the content of the generated data. The region and language refinements will be applied to this data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:28:21.542834Z",
     "start_time": "2024-05-02T08:27:48.524063Z"
    }
   },
   "source": [
    "generated_data = DataAugmentorObj.preview_output_sample(query=query, region=region, language=language, task_specifications=updated_task_specifications)\n",
    "print(generated_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate 10 Sample synthetic items(records in a table or the equivalent elements in unstructured text) for the data described below. In the following structure, but with different values.For the following task: Data description:  ; The required region: France ; All texts should be translated to French language. ; The guidance given by an expert: 1. Relevant Columns:\n",
      "- Identifiant de l'utilisateur (identifiant unique)\n",
      "- Nom d'utilisateur (texte libre)\n",
      "- Bio (texte libre)\n",
      "- Lieu (catégorique)\n",
      "- Nombre d'abonnés (numérique)\n",
      "- Nombre de personnes suivies (numérique)\n",
      "- Nombre de tweets (numérique)\n",
      "- Date de création du compte (datetime)\n",
      "- Identifiant du tweet (identifiant unique)\n",
      "- Texte du tweet (texte libre)\n",
      "- Date du tweet (datetime)\n",
      "- Nombre de \"J'aime\" (numérique)\n",
      "- Nombre de retweets (numérique)\n",
      "- Nombre de réponses (numérique)\n",
      "\n",
      "2. Column Details:\n",
      "- Identifiant de l'utilisateur : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Nom d'utilisateur : longueur moyenne du texte - 7, écart-type de la longueur du texte - 2\n",
      "- Lieu : catégories - [États-Unis, Royaume-Uni, Canada, Australie, Autre], probabilités - [0.3, 0.2, 0.1, 0.1, 0.3]\n",
      "- Nombre d'abonnés : distribution - asymétrique positive, moyenne - 1000, écart-type - 500, min - 100, max - 5000\n",
      "- Nombre de personnes suivies : distribution - asymétrique positive, moyenne - 500, écart-type - 200, min - 50, max - 2000\n",
      "- Nombre de tweets : distribution - normalement distribuée, moyenne - 2000, écart-type - 1000, min - 500, max - 5000\n",
      "- Date de création du compte : format - jj/mm/aaaa, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 2 ans, écart-type - 1 an\n",
      "- Identifiant du tweet : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Texte du tweet : longueur moyenne du texte - 100, écart-type de la longueur du texte - 20\n",
      "- Date du tweet : format - jj/mm/aaaa hh:mm:ss, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 1 jour, écart-type - 6 heures\n",
      "- Nombre de \"J'aime\" : distribution - asymétrique positive, moyenne - 100, écart-type - 50, min - 10, max - 500\n",
      "- Nombre de retweets : distribution - asymétrique positive, moyenne - 50, écart-type - 20, min - 5, max - 200\n",
      "- Nombre de réponses : distribution - normalement distribuée, moyenne - 20, écart-type - 10, min - 1, max - 100\n",
      "\n",
      "3. Constraints and Dependencies:\n",
      "- L'identifiant de l'utilisateur est unique et doit être maintenu comme clé primaire.\n",
      "- L'identifiant du tweet est unique et doit être maintenu comme clé primaire.\n",
      "- Les compteurs d'engagements (likes, retweets, réponses) doivent dépendre du tweet correspondant.\n",
      "\n",
      "4. Fields for LLM:\n",
      "- La génération du contenu textuel des tweets pourrait bénéficier d'un modèle de langage pour assurer la cohérence et la pertinence.\n",
      "\n",
      "5. Sequential Nature:\n",
      "- Cette table a une nature séquentielle basée sur l'ordre chronologique des dates de création des comptes et des dates des tweets...Required Structure: {\n",
      "    \"twitter_api\": [\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u2387hj10\",\n",
      "                \"username\": \"user123\",\n",
      "                \"bio\": \"Lorem ipsum dolor sit amet\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1200,\n",
      "                \"following_count\": 600,\n",
      "                \"tweet_count\": 2300,\n",
      "                \"account_creation_date\": \"15/06/2015\",\n",
      "                \"tweet_id\": \"t4390dfg\",\n",
      "                \"tweet_text\": \"This is a sample tweet text\",\n",
      "                \"tweet_date\": \"20/08/2015 14:30:00\",\n",
      "                \"likes_count\": 110,\n",
      "                \"retweets_count\": 55,\n",
      "                \"replies_count\": 25\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u3985klmn\",\n",
      "                \"username\": \"abc_user\",\n",
      "                \"bio\": \"Consectetur adipiscing elit\",\n",
      "                \"location\": \"UK\",\n",
      "                \"followers_count\": 1500,\n",
      "                \"following_count\": 700,\n",
      "                \"tweet_count\": 2600,\n",
      "                \"account_creation_date\": \"25/03/2012\",\n",
      "                \"tweet_id\": \"t5678qwe\",\n",
      "                \"tweet_text\": \"Another sample tweet text\",\n",
      "                \"tweet_date\": \"10/10/2012 09:45:00\",\n",
      "                \"likes_count\": 90,\n",
      "                \"retweets_count\": 45,\n",
      "                \"replies_count\": 20\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u4567opqr\",\n",
      "                \"username\": \"test_user\",\n",
      "                \"bio\": \"Sed do eiusmod tempor incididunt\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1300,\n",
      "                \"following_count\": 550,\n",
      "                \"tweet_count\": 2100,\n",
      "                \"account_creation_date\": \"08/12/2018\",\n",
      "                \"tweet_id\": \"t6789zxc\",\n",
      "                \"tweet_text\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit\",\n",
      "                \"tweet_date\": \"05/05/2019 17:20:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 60,\n",
      "                \"replies_count\": 30\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}.The previous generated batch (maintain sequential patterns if needed): unknown.Add Randomness: 0.7124913875087483.It is critical that you don't repeat the same values every time you get this prompt.\n",
      "Generated Output:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'twitter_api':    input.user_id    input.username                         input.bio  \\\n",
      "0       u8712abc           user456  Sed do eiusmod tempor incididunt   \n",
      "1      u5678defg          new_user        Lorem ipsum dolor sit amet   \n",
      "2      u3456ijkl  utilisateur_test       Consectetur adipiscing elit   \n",
      "3      u9876mnop   abc_utilisateur  Sed do eiusmod tempor incididunt   \n",
      "4      u2345qrst           test123        Lorem ipsum dolor sit amet   \n",
      "5      u7890hijk         user_test       Consectetur adipiscing elit   \n",
      "6      u9012uvwx   xyz_utilisateur  Sed do eiusmod tempor incididunt   \n",
      "7      u6789klmo   utilisateur_xyz        Lorem ipsum dolor sit amet   \n",
      "8      u1234abcd         abcd_user       Consectetur adipiscing elit   \n",
      "9      u3456efgh      test_user123  Sed do eiusmod tempor incididunt   \n",
      "10    u2345ijklm   utilisateur_123        Lorem ipsum dolor sit amet   \n",
      "\n",
      "   input.location  input.followers_count  input.following_count  \\\n",
      "0          Canada                   1400                    700   \n",
      "1             USA                   1100                    550   \n",
      "2              UK                   1600                    650   \n",
      "3          Canada                   1200                    600   \n",
      "4             USA                   1300                    500   \n",
      "5              UK                   1400                    700   \n",
      "6          Canada                   1500                    650   \n",
      "7             USA                   1100                    550   \n",
      "8              UK                   1600                    650   \n",
      "9          Canada                   1300                    600   \n",
      "10            USA                   1400                    700   \n",
      "\n",
      "    input.tweet_count input.account_creation_date input.tweet_id  \\\n",
      "0                2400                  03/09/2013       t8901vbn   \n",
      "1                2200                  18/07/2011       t6789rty   \n",
      "2                2800                  29/05/2014       t1234fgh   \n",
      "3                2600                  22/08/2016       t3456mnb   \n",
      "4                2300                  11/11/2018       t4567zxc   \n",
      "5                2700                  04/04/2015       t2345qwe   \n",
      "6                2500                  17/02/2017       t5678yhn   \n",
      "7                2400                  31/01/2019       t7890iop   \n",
      "8                2600                  09/06/2016       t1234bnm   \n",
      "9                2500                  14/03/2014       t4567vbn   \n",
      "10               2700                  23/12/2017       t5678rty   \n",
      "\n",
      "                              input.tweet_text     input.tweet_date  \\\n",
      "0           Voici un exemple de texte de tweet  12/11/2013 11:50:00   \n",
      "1          Encore un exemple de texte de tweet  25/04/2012 08:15:00   \n",
      "2           Un autre exemple de texte de tweet  15/09/2014 16:40:00   \n",
      "3              Texte de tweet de démonstration  30/01/2017 10:25:00   \n",
      "4               Tweet de synthèse pour exemple  08/03/2019 13:15:00   \n",
      "5          Exemple de texte de tweet aléatoire  22/10/2015 09:30:00   \n",
      "6   Texte de tweet généré de manière aléatoire  02/06/2017 14:45:00   \n",
      "7            Texte de tweet pour démonstration  18/05/2019 12:10:00   \n",
      "8                     Texte de tweet aléatoire  05/12/2016 15:20:00   \n",
      "9                  Texte de tweet pour exemple  28/07/2014 11:40:00   \n",
      "10            Exemple de texte de tweet généré  10/09/2018 10:35:00   \n",
      "\n",
      "    input.likes_count  input.retweets_count  input.replies_count output.status  \n",
      "0                 130                    65                   35       success  \n",
      "1                 100                    50                   20       success  \n",
      "2                 120                    55                   30       success  \n",
      "3                 110                    60                   25       success  \n",
      "4                 130                    70                   30       success  \n",
      "5                 120                    65                   35       success  \n",
      "6                 140                    70                   40       success  \n",
      "7                 110                    60                   25       success  \n",
      "8                 120                    55                   30       success  \n",
      "9                 130                    65                   35       success  \n",
      "10                140                    70                   40       success  }\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Full Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the full dataset, use the consumeData method. Specify your query (if any), optionaly the region and language, and the number of records you wish to generate. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:29:41.123306Z",
     "start_time": "2024-05-02T08:28:21.543843Z"
    }
   },
   "source": [
    "# Initialize Generator\n",
    "#generatorObj = DataAugmentor(llm=llm, structure=dataStructureSample)\n",
    "query = \"Taylor Swift fans\"\n",
    "generated_data = DataAugmentorObj.generate_data( query=query, region=region, language=language, task_specifications=updated_task_specifications, num_records=15) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate 10 Sample synthetic items(records in a table or the equivalent elements in unstructured text) for the data described below. In the following structure, but with different values.For the following task: Data description: Taylor Swift fans ; The required region: France ; All texts should be translated to French language. ; The guidance given by an expert: 1. Relevant Columns:\n",
      "- Identifiant de l'utilisateur (identifiant unique)\n",
      "- Nom d'utilisateur (texte libre)\n",
      "- Bio (texte libre)\n",
      "- Lieu (catégorique)\n",
      "- Nombre d'abonnés (numérique)\n",
      "- Nombre de personnes suivies (numérique)\n",
      "- Nombre de tweets (numérique)\n",
      "- Date de création du compte (datetime)\n",
      "- Identifiant du tweet (identifiant unique)\n",
      "- Texte du tweet (texte libre)\n",
      "- Date du tweet (datetime)\n",
      "- Nombre de \"J'aime\" (numérique)\n",
      "- Nombre de retweets (numérique)\n",
      "- Nombre de réponses (numérique)\n",
      "\n",
      "2. Column Details:\n",
      "- Identifiant de l'utilisateur : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Nom d'utilisateur : longueur moyenne du texte - 7, écart-type de la longueur du texte - 2\n",
      "- Lieu : catégories - [États-Unis, Royaume-Uni, Canada, Australie, Autre], probabilités - [0.3, 0.2, 0.1, 0.1, 0.3]\n",
      "- Nombre d'abonnés : distribution - asymétrique positive, moyenne - 1000, écart-type - 500, min - 100, max - 5000\n",
      "- Nombre de personnes suivies : distribution - asymétrique positive, moyenne - 500, écart-type - 200, min - 50, max - 2000\n",
      "- Nombre de tweets : distribution - normalement distribuée, moyenne - 2000, écart-type - 1000, min - 500, max - 5000\n",
      "- Date de création du compte : format - jj/mm/aaaa, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 2 ans, écart-type - 1 an\n",
      "- Identifiant du tweet : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Texte du tweet : longueur moyenne du texte - 100, écart-type de la longueur du texte - 20\n",
      "- Date du tweet : format - jj/mm/aaaa hh:mm:ss, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 1 jour, écart-type - 6 heures\n",
      "- Nombre de \"J'aime\" : distribution - asymétrique positive, moyenne - 100, écart-type - 50, min - 10, max - 500\n",
      "- Nombre de retweets : distribution - asymétrique positive, moyenne - 50, écart-type - 20, min - 5, max - 200\n",
      "- Nombre de réponses : distribution - normalement distribuée, moyenne - 20, écart-type - 10, min - 1, max - 100\n",
      "\n",
      "3. Constraints and Dependencies:\n",
      "- L'identifiant de l'utilisateur est unique et doit être maintenu comme clé primaire.\n",
      "- L'identifiant du tweet est unique et doit être maintenu comme clé primaire.\n",
      "- Les compteurs d'engagements (likes, retweets, réponses) doivent dépendre du tweet correspondant.\n",
      "\n",
      "4. Fields for LLM:\n",
      "- La génération du contenu textuel des tweets pourrait bénéficier d'un modèle de langage pour assurer la cohérence et la pertinence.\n",
      "\n",
      "5. Sequential Nature:\n",
      "- Cette table a une nature séquentielle basée sur l'ordre chronologique des dates de création des comptes et des dates des tweets...Required Structure: {\n",
      "    \"twitter_api\": [\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u2387hj10\",\n",
      "                \"username\": \"user123\",\n",
      "                \"bio\": \"Lorem ipsum dolor sit amet\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1200,\n",
      "                \"following_count\": 600,\n",
      "                \"tweet_count\": 2300,\n",
      "                \"account_creation_date\": \"15/06/2015\",\n",
      "                \"tweet_id\": \"t4390dfg\",\n",
      "                \"tweet_text\": \"This is a sample tweet text\",\n",
      "                \"tweet_date\": \"20/08/2015 14:30:00\",\n",
      "                \"likes_count\": 110,\n",
      "                \"retweets_count\": 55,\n",
      "                \"replies_count\": 25\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u3985klmn\",\n",
      "                \"username\": \"abc_user\",\n",
      "                \"bio\": \"Consectetur adipiscing elit\",\n",
      "                \"location\": \"UK\",\n",
      "                \"followers_count\": 1500,\n",
      "                \"following_count\": 700,\n",
      "                \"tweet_count\": 2600,\n",
      "                \"account_creation_date\": \"25/03/2012\",\n",
      "                \"tweet_id\": \"t5678qwe\",\n",
      "                \"tweet_text\": \"Another sample tweet text\",\n",
      "                \"tweet_date\": \"10/10/2012 09:45:00\",\n",
      "                \"likes_count\": 90,\n",
      "                \"retweets_count\": 45,\n",
      "                \"replies_count\": 20\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u4567opqr\",\n",
      "                \"username\": \"test_user\",\n",
      "                \"bio\": \"Sed do eiusmod tempor incididunt\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1300,\n",
      "                \"following_count\": 550,\n",
      "                \"tweet_count\": 2100,\n",
      "                \"account_creation_date\": \"08/12/2018\",\n",
      "                \"tweet_id\": \"t6789zxc\",\n",
      "                \"tweet_text\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit\",\n",
      "                \"tweet_date\": \"05/05/2019 17:20:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 60,\n",
      "                \"replies_count\": 30\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}.The previous generated batch (maintain sequential patterns if needed): {\n",
      "    \"twitter_api\": [\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u8712abc\",\n",
      "                \"username\": \"user456\",\n",
      "                \"bio\": \"Sed do eiusmod tempor incididunt\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1400,\n",
      "                \"following_count\": 700,\n",
      "                \"tweet_count\": 2400,\n",
      "                \"account_creation_date\": \"03/09/2013\",\n",
      "                \"tweet_id\": \"t8901vbn\",\n",
      "                \"tweet_text\": \"Voici un exemple de texte de tweet\",\n",
      "                \"tweet_date\": \"12/11/2013 11:50:00\",\n",
      "                \"likes_count\": 130,\n",
      "                \"retweets_count\": 65,\n",
      "                \"replies_count\": 35\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u5678defg\",\n",
      "                \"username\": \"new_user\",\n",
      "                \"bio\": \"Lorem ipsum dolor sit amet\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1100,\n",
      "                \"following_count\": 550,\n",
      "                \"tweet_count\": 2200,\n",
      "                \"account_creation_date\": \"18/07/2011\",\n",
      "                \"tweet_id\": \"t6789rty\",\n",
      "                \"tweet_text\": \"Encore un exemple de texte de tweet\",\n",
      "                \"tweet_date\": \"25/04/2012 08:15:00\",\n",
      "                \"likes_count\": 100,\n",
      "                \"retweets_count\": 50,\n",
      "                \"replies_count\": 20\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u3456ijkl\",\n",
      "                \"username\": \"utilisateur_test\",\n",
      "                \"bio\": \"Consectetur adipiscing elit\",\n",
      "                \"location\": \"UK\",\n",
      "                \"followers_count\": 1600,\n",
      "                \"following_count\": 650,\n",
      "                \"tweet_count\": 2800,\n",
      "                \"account_creation_date\": \"29/05/2014\",\n",
      "                \"tweet_id\": \"t1234fgh\",\n",
      "                \"tweet_text\": \"Un autre exemple de texte de tweet\",\n",
      "                \"tweet_date\": \"15/09/2014 16:40:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 55,\n",
      "                \"replies_count\": 30\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u9876mnop\",\n",
      "                \"username\": \"abc_utilisateur\",\n",
      "                \"bio\": \"Sed do eiusmod tempor incididunt\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1200,\n",
      "                \"following_count\": 600,\n",
      "                \"tweet_count\": 2600,\n",
      "                \"account_creation_date\": \"22/08/2016\",\n",
      "                \"tweet_id\": \"t3456mnb\",\n",
      "                \"tweet_text\": \"Texte de tweet de démonstration\",\n",
      "                \"tweet_date\": \"30/01/2017 10:25:00\",\n",
      "                \"likes_count\": 110,\n",
      "                \"retweets_count\": 60,\n",
      "                \"replies_count\": 25\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u2345qrst\",\n",
      "                \"username\": \"test123\",\n",
      "                \"bio\": \"Lorem ipsum dolor sit amet\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1300,\n",
      "                \"following_count\": 500,\n",
      "                \"tweet_count\": 2300,\n",
      "                \"account_creation_date\": \"11/11/2018\",\n",
      "                \"tweet_id\": \"t4567zxc\",\n",
      "                \"tweet_text\": \"Tweet de synthèse pour exemple\",\n",
      "                \"tweet_date\": \"08/03/2019 13:15:00\",\n",
      "                \"likes_count\": 130,\n",
      "                \"retweets_count\": 70,\n",
      "                \"replies_count\": 30\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u7890hijk\",\n",
      "                \"username\": \"user_test\",\n",
      "                \"bio\": \"Consectetur adipiscing elit\",\n",
      "                \"location\": \"UK\",\n",
      "                \"followers_count\": 1400,\n",
      "                \"following_count\": 700,\n",
      "                \"tweet_count\": 2700,\n",
      "                \"account_creation_date\": \"04/04/2015\",\n",
      "                \"tweet_id\": \"t2345qwe\",\n",
      "                \"tweet_text\": \"Exemple de texte de tweet aléatoire\",\n",
      "                \"tweet_date\": \"22/10/2015 09:30:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 65,\n",
      "                \"replies_count\": 35\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u9012uvwx\",\n",
      "                \"username\": \"xyz_utilisateur\",\n",
      "                \"bio\": \"Sed do eiusmod tempor incididunt\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1500,\n",
      "                \"following_count\": 650,\n",
      "                \"tweet_count\": 2500,\n",
      "                \"account_creation_date\": \"17/02/2017\",\n",
      "                \"tweet_id\": \"t5678yhn\",\n",
      "                \"tweet_text\": \"Texte de tweet généré de manière aléatoire\",\n",
      "                \"tweet_date\": \"02/06/2017 14:45:00\",\n",
      "                \"likes_count\": 140,\n",
      "                \"retweets_count\": 70,\n",
      "                \"replies_count\": 40\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u6789klmo\",\n",
      "                \"username\": \"utilisateur_xyz\",\n",
      "                \"bio\": \"Lorem ipsum dolor sit amet\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1100,\n",
      "                \"following_count\": 550,\n",
      "                \"tweet_count\": 2400,\n",
      "                \"account_creation_date\": \"31/01/2019\",\n",
      "                \"tweet_id\": \"t7890iop\",\n",
      "                \"tweet_text\": \"Texte de tweet pour démonstration\",\n",
      "                \"tweet_date\": \"18/05/2019 12:10:00\",\n",
      "                \"likes_count\": 110,\n",
      "                \"retweets_count\": 60,\n",
      "                \"replies_count\": 25\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u1234abcd\",\n",
      "                \"username\": \"abcd_user\",\n",
      "                \"bio\": \"Consectetur adipiscing elit\",\n",
      "                \"location\": \"UK\",\n",
      "                \"followers_count\": 1600,\n",
      "                \"following_count\": 650,\n",
      "                \"tweet_count\": 2600,\n",
      "                \"account_creation_date\": \"09/06/2016\",\n",
      "                \"tweet_id\": \"t1234bnm\",\n",
      "                \"tweet_text\": \"Texte de tweet aléatoire\",\n",
      "                \"tweet_date\": \"05/12/2016 15:20:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 55,\n",
      "                \"replies_count\": 30\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u3456efgh\",\n",
      "                \"username\": \"test_user123\",\n",
      "                \"bio\": \"Sed do eiusmod tempor incididunt\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1300,\n",
      "                \"following_count\": 600,\n",
      "                \"tweet_count\": 2500,\n",
      "                \"account_creation_date\": \"14/03/2014\",\n",
      "                \"tweet_id\": \"t4567vbn\",\n",
      "                \"tweet_text\": \"Texte de tweet pour exemple\",\n",
      "                \"tweet_date\": \"28/07/2014 11:40:00\",\n",
      "                \"likes_count\": 130,\n",
      "                \"retweets_count\": 65,\n",
      "                \"replies_count\": 35\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u2345ijklm\",\n",
      "                \"username\": \"utilisateur_123\",\n",
      "                \"bio\": \"Lorem ipsum dolor sit amet\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1400,\n",
      "                \"following_count\": 700,\n",
      "                \"tweet_count\": 2700,\n",
      "                \"account_creation_date\": \"23/12/2017\",\n",
      "                \"tweet_id\": \"t5678rty\",\n",
      "                \"tweet_text\": \"Exemple de texte de tweet généré\",\n",
      "                \"tweet_date\": \"10/09/2018 10:35:00\",\n",
      "                \"likes_count\": 140,\n",
      "                \"retweets_count\": 70,\n",
      "                \"replies_count\": 40\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}.Add Randomness: 0.33164185558074755.It is critical that you don't repeat the same values every time you get this prompt.\n",
      "Generated Output:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate 10 Sample synthetic items(records in a table or the equivalent elements in unstructured text) for the data described below. In the following structure, but with different values.For the following task: Data description: Taylor Swift fans ; The required region: France ; All texts should be translated to French language. ; The guidance given by an expert: 1. Relevant Columns:\n",
      "- Identifiant de l'utilisateur (identifiant unique)\n",
      "- Nom d'utilisateur (texte libre)\n",
      "- Bio (texte libre)\n",
      "- Lieu (catégorique)\n",
      "- Nombre d'abonnés (numérique)\n",
      "- Nombre de personnes suivies (numérique)\n",
      "- Nombre de tweets (numérique)\n",
      "- Date de création du compte (datetime)\n",
      "- Identifiant du tweet (identifiant unique)\n",
      "- Texte du tweet (texte libre)\n",
      "- Date du tweet (datetime)\n",
      "- Nombre de \"J'aime\" (numérique)\n",
      "- Nombre de retweets (numérique)\n",
      "- Nombre de réponses (numérique)\n",
      "\n",
      "2. Column Details:\n",
      "- Identifiant de l'utilisateur : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Nom d'utilisateur : longueur moyenne du texte - 7, écart-type de la longueur du texte - 2\n",
      "- Lieu : catégories - [États-Unis, Royaume-Uni, Canada, Australie, Autre], probabilités - [0.3, 0.2, 0.1, 0.1, 0.3]\n",
      "- Nombre d'abonnés : distribution - asymétrique positive, moyenne - 1000, écart-type - 500, min - 100, max - 5000\n",
      "- Nombre de personnes suivies : distribution - asymétrique positive, moyenne - 500, écart-type - 200, min - 50, max - 2000\n",
      "- Nombre de tweets : distribution - normalement distribuée, moyenne - 2000, écart-type - 1000, min - 500, max - 5000\n",
      "- Date de création du compte : format - jj/mm/aaaa, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 2 ans, écart-type - 1 an\n",
      "- Identifiant du tweet : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Texte du tweet : longueur moyenne du texte - 100, écart-type de la longueur du texte - 20\n",
      "- Date du tweet : format - jj/mm/aaaa hh:mm:ss, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 1 jour, écart-type - 6 heures\n",
      "- Nombre de \"J'aime\" : distribution - asymétrique positive, moyenne - 100, écart-type - 50, min - 10, max - 500\n",
      "- Nombre de retweets : distribution - asymétrique positive, moyenne - 50, écart-type - 20, min - 5, max - 200\n",
      "- Nombre de réponses : distribution - normalement distribuée, moyenne - 20, écart-type - 10, min - 1, max - 100\n",
      "\n",
      "3. Constraints and Dependencies:\n",
      "- L'identifiant de l'utilisateur est unique et doit être maintenu comme clé primaire.\n",
      "- L'identifiant du tweet est unique et doit être maintenu comme clé primaire.\n",
      "- Les compteurs d'engagements (likes, retweets, réponses) doivent dépendre du tweet correspondant.\n",
      "\n",
      "4. Fields for LLM:\n",
      "- La génération du contenu textuel des tweets pourrait bénéficier d'un modèle de langage pour assurer la cohérence et la pertinence.\n",
      "\n",
      "5. Sequential Nature:\n",
      "- Cette table a une nature séquentielle basée sur l'ordre chronologique des dates de création des comptes et des dates des tweets...Required Structure: {\n",
      "    \"twitter_api\": [\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u2387hj10\",\n",
      "                \"username\": \"user123\",\n",
      "                \"bio\": \"Lorem ipsum dolor sit amet\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1200,\n",
      "                \"following_count\": 600,\n",
      "                \"tweet_count\": 2300,\n",
      "                \"account_creation_date\": \"15/06/2015\",\n",
      "                \"tweet_id\": \"t4390dfg\",\n",
      "                \"tweet_text\": \"This is a sample tweet text\",\n",
      "                \"tweet_date\": \"20/08/2015 14:30:00\",\n",
      "                \"likes_count\": 110,\n",
      "                \"retweets_count\": 55,\n",
      "                \"replies_count\": 25\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u3985klmn\",\n",
      "                \"username\": \"abc_user\",\n",
      "                \"bio\": \"Consectetur adipiscing elit\",\n",
      "                \"location\": \"UK\",\n",
      "                \"followers_count\": 1500,\n",
      "                \"following_count\": 700,\n",
      "                \"tweet_count\": 2600,\n",
      "                \"account_creation_date\": \"25/03/2012\",\n",
      "                \"tweet_id\": \"t5678qwe\",\n",
      "                \"tweet_text\": \"Another sample tweet text\",\n",
      "                \"tweet_date\": \"10/10/2012 09:45:00\",\n",
      "                \"likes_count\": 90,\n",
      "                \"retweets_count\": 45,\n",
      "                \"replies_count\": 20\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u4567opqr\",\n",
      "                \"username\": \"test_user\",\n",
      "                \"bio\": \"Sed do eiusmod tempor incididunt\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1300,\n",
      "                \"following_count\": 550,\n",
      "                \"tweet_count\": 2100,\n",
      "                \"account_creation_date\": \"08/12/2018\",\n",
      "                \"tweet_id\": \"t6789zxc\",\n",
      "                \"tweet_text\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit\",\n",
      "                \"tweet_date\": \"05/05/2019 17:20:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 60,\n",
      "                \"replies_count\": 30\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}.The previous generated batch (maintain sequential patterns if needed): {\n",
      "    \"twitter_api\": [\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u0987qrst\",\n",
      "                \"username\": \"fan123\",\n",
      "                \"bio\": \"Amateur de Taylor Swift depuis des années\",\n",
      "                \"location\": \"Australie\",\n",
      "                \"followers_count\": 1250,\n",
      "                \"following_count\": 550,\n",
      "                \"tweet_count\": 2200,\n",
      "                \"account_creation_date\": \"12/08/2014\",\n",
      "                \"tweet_id\": \"t9876hjk\",\n",
      "                \"tweet_text\": \"J'adore la nouvelle chanson de Taylor Swift!\",\n",
      "                \"tweet_date\": \"18/10/2014 16:25:00\",\n",
      "                \"likes_count\": 115,\n",
      "                \"retweets_count\": 60,\n",
      "                \"replies_count\": 30\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u2345ghij\",\n",
      "                \"username\": \"swiftie456\",\n",
      "                \"bio\": \"Passionné par la musique de Taylor Swift\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1150,\n",
      "                \"following_count\": 600,\n",
      "                \"tweet_count\": 2400,\n",
      "                \"account_creation_date\": \"23/06/2016\",\n",
      "                \"tweet_id\": \"t6543mnb\",\n",
      "                \"tweet_text\": \"La voix de Taylor Swift est incroyable!\",\n",
      "                \"tweet_date\": \"29/08/2016 14:40:00\",\n",
      "                \"likes_count\": 105,\n",
      "                \"retweets_count\": 50,\n",
      "                \"replies_count\": 25\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u3456wxyz\",\n",
      "                \"username\": \"musiclover789\",\n",
      "                \"bio\": \"Fervent admirateur de Taylor Swift\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1350,\n",
      "                \"following_count\": 620,\n",
      "                \"tweet_count\": 2300,\n",
      "                \"account_creation_date\": \"17/04/2015\",\n",
      "                \"tweet_id\": \"t8765vbn\",\n",
      "                \"tweet_text\": \"Taylor Swift est une véritable icône de la musique\",\n",
      "                \"tweet_date\": \"25/07/2015 12:15:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 55,\n",
      "                \"replies_count\": 28\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u4567mnop\",\n",
      "                \"username\": \"taylorswiftfan\",\n",
      "                \"bio\": \"Supporteur dévoué de Taylor Swift\",\n",
      "                \"location\": \"Royaume-Uni\",\n",
      "                \"followers_count\": 1300,\n",
      "                \"following_count\": 570,\n",
      "                \"tweet_count\": 2500,\n",
      "                \"account_creation_date\": \"08/10/2013\",\n",
      "                \"tweet_id\": \"t7654nmq\",\n",
      "                \"tweet_text\": \"L'album de Taylor Swift est incroyable!\",\n",
      "                \"tweet_date\": \"14/12/2013 10:30:00\",\n",
      "                \"likes_count\": 110,\n",
      "                \"retweets_count\": 65,\n",
      "                \"replies_count\": 32\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u5678ijkl\",\n",
      "                \"username\": \"melodiefan\",\n",
      "                \"bio\": \"Écouter la musique de Taylor Swift me rend heureux\",\n",
      "                \"location\": \"Autre\",\n",
      "                \"followers_count\": 1200,\n",
      "                \"following_count\": 590,\n",
      "                \"tweet_count\": 2600,\n",
      "                \"account_creation_date\": \"05/02/2012\",\n",
      "                \"tweet_id\": \"t6543klo\",\n",
      "                \"tweet_text\": \"Taylor Swift a une voix incroyable!\",\n",
      "                \"tweet_date\": \"11/04/2012 09:20:00\",\n",
      "                \"likes_count\": 100,\n",
      "                \"retweets_count\": 60,\n",
      "                \"replies_count\": 27\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u6789pqrs\",\n",
      "                \"username\": \"taytaylover\",\n",
      "                \"bio\": \"Fan absolu de Taylor Swift depuis le premier jour\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1250,\n",
      "                \"following_count\": 630,\n",
      "                \"tweet_count\": 2300,\n",
      "                \"account_creation_date\": \"22/11/2017\",\n",
      "                \"tweet_id\": \"t5432qwe\",\n",
      "                \"tweet_text\": \"J'attends avec impatience le prochain concert de Taylor Swift!\",\n",
      "                \"tweet_date\": \"28/01/2018 11:45:00\",\n",
      "                \"likes_count\": 115,\n",
      "                \"retweets_count\": 55,\n",
      "                \"replies_count\": 29\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u7890tuvw\",\n",
      "                \"username\": \"swiftiesunite\",\n",
      "                \"bio\": \"Rejoignez-moi dans ma passion pour Taylor Swift\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1350,\n",
      "                \"following_count\": 610,\n",
      "                \"tweet_count\": 2400,\n",
      "                \"account_creation_date\": \"28/09/2014\",\n",
      "                \"tweet_id\": \"t4321iop\",\n",
      "                \"tweet_text\": \"La musique de Taylor Swift me parle toujours\",\n",
      "                \"tweet_date\": \"05/12/2014 15:10:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 52,\n",
      "                \"replies_count\": 26\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u8901efgh\",\n",
      "                \"username\": \"taylorlover\",\n",
      "                \"bio\": \"Taylor Swift occupe une place spéciale dans mon cœur\",\n",
      "                \"location\": \"Royaume-Uni\",\n",
      "                \"followers_count\": 1300,\n",
      "                \"following_count\": 580,\n",
      "                \"tweet_count\": 2200,\n",
      "                \"account_creation_date\": \"15/07/2016\",\n",
      "                \"tweet_id\": \"t3210klm\",\n",
      "                \"tweet_text\": \"Les paroles des chansons de Taylor Swift sont si inspirantes\",\n",
      "                \"tweet_date\": \"21/09/2016 14:20:00\",\n",
      "                \"likes_count\": 110,\n",
      "                \"retweets_count\": 57,\n",
      "                \"replies_count\": 31\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u9012uvwx\",\n",
      "                \"username\": \"taylorswiftforever\",\n",
      "                \"bio\": \"Taylor Swift est ma source de bonheur quotidienne\",\n",
      "                \"location\": \"États-Unis\",\n",
      "                \"followers_count\": 1400,\n",
      "                \"following_count\": 640,\n",
      "                \"tweet_count\": 2600,\n",
      "                \"account_creation_date\": \"19/05/2015\",\n",
      "                \"tweet_id\": \"t2109mno\",\n",
      "                \"tweet_text\": \"Peu importe la situation, la musique de Taylor Swift est là pour moi\",\n",
      "                \"tweet_date\": \"25/07/2015 10:50:00\",\n",
      "                \"likes_count\": 125,\n",
      "                \"retweets_count\": 58,\n",
      "                \"replies_count\": 28\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u0123abcd\",\n",
      "                \"username\": \"taylorswiftaddict\",\n",
      "                \"bio\": \"Obsédé par tout ce qui concerne Taylor Swift\",\n",
      "                \"location\": \"Autre\",\n",
      "                \"followers_count\": 1250,\n",
      "                \"following_count\": 600,\n",
      "                \"tweet_count\": 2500,\n",
      "                \"account_creation_date\": \"11/02/2013\",\n",
      "                \"tweet_id\": \"t1098pqr\",\n",
      "                \"tweet_text\": \"Ma journée commence toujours en écoutant Taylor Swift\",\n",
      "                \"tweet_date\": \"17/04/2013 09:30:00\",\n",
      "                \"likes_count\": 115,\n",
      "                \"retweets_count\": 53,\n",
      "                \"replies_count\": 27\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}.Add Randomness: 0.377888087959967.It is critical that you don't repeat the same values every time you get this prompt.\n",
      "Generated Output:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Full Output in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more efficient data generation, especially when dealing with large datasets or multiple requests, our package supports parallel processing. This section covers how to utilize the consumeDataInParallel method of the DataConsumer class to generate your dataset asynchronously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Parallel Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure smooth parallel execution, especially within environments that don't natively support asynchronous operations (like Jupyter notebooks), we use nest_asyncio. This module allows asyncio to run inside environments with their own event loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:29:41.129053Z",
     "start_time": "2024-05-02T08:29:41.124315Z"
    }
   },
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Full Output in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate data in parallel, use the generate_data_in_parallel coroutine. This method allows you to specify the query (if any), the number of records, region, and language, similarly to consumeData, but executes multiple data generation tasks concurrently.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:30:22.245221Z",
     "start_time": "2024-05-02T08:29:41.130059Z"
    }
   },
   "source": [
    "import asyncio\n",
    "generated_data = asyncio.run(DataAugmentorObj.generate_data_in_parallel(query = \"\", records=20, task_specifications=updated_task_specifications, region=region, language=language))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate 10 Sample synthetic items(records in a table or the equivalent elements in unstructured text) for the data described below. In the following structure, but with different values.For the following task: Data description:  ; The required region: France ; All texts should be translated to French language. ; The guidance given by an expert: 1. Relevant Columns:\n",
      "- Identifiant de l'utilisateur (identifiant unique)\n",
      "- Nom d'utilisateur (texte libre)\n",
      "- Bio (texte libre)\n",
      "- Lieu (catégorique)\n",
      "- Nombre d'abonnés (numérique)\n",
      "- Nombre de personnes suivies (numérique)\n",
      "- Nombre de tweets (numérique)\n",
      "- Date de création du compte (datetime)\n",
      "- Identifiant du tweet (identifiant unique)\n",
      "- Texte du tweet (texte libre)\n",
      "- Date du tweet (datetime)\n",
      "- Nombre de \"J'aime\" (numérique)\n",
      "- Nombre de retweets (numérique)\n",
      "- Nombre de réponses (numérique)\n",
      "\n",
      "2. Column Details:\n",
      "- Identifiant de l'utilisateur : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Nom d'utilisateur : longueur moyenne du texte - 7, écart-type de la longueur du texte - 2\n",
      "- Lieu : catégories - [États-Unis, Royaume-Uni, Canada, Australie, Autre], probabilités - [0.3, 0.2, 0.1, 0.1, 0.3]\n",
      "- Nombre d'abonnés : distribution - asymétrique positive, moyenne - 1000, écart-type - 500, min - 100, max - 5000\n",
      "- Nombre de personnes suivies : distribution - asymétrique positive, moyenne - 500, écart-type - 200, min - 50, max - 2000\n",
      "- Nombre de tweets : distribution - normalement distribuée, moyenne - 2000, écart-type - 1000, min - 500, max - 5000\n",
      "- Date de création du compte : format - jj/mm/aaaa, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 2 ans, écart-type - 1 an\n",
      "- Identifiant du tweet : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Texte du tweet : longueur moyenne du texte - 100, écart-type de la longueur du texte - 20\n",
      "- Date du tweet : format - jj/mm/aaaa hh:mm:ss, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 1 jour, écart-type - 6 heures\n",
      "- Nombre de \"J'aime\" : distribution - asymétrique positive, moyenne - 100, écart-type - 50, min - 10, max - 500\n",
      "- Nombre de retweets : distribution - asymétrique positive, moyenne - 50, écart-type - 20, min - 5, max - 200\n",
      "- Nombre de réponses : distribution - normalement distribuée, moyenne - 20, écart-type - 10, min - 1, max - 100\n",
      "\n",
      "3. Constraints and Dependencies:\n",
      "- L'identifiant de l'utilisateur est unique et doit être maintenu comme clé primaire.\n",
      "- L'identifiant du tweet est unique et doit être maintenu comme clé primaire.\n",
      "- Les compteurs d'engagements (likes, retweets, réponses) doivent dépendre du tweet correspondant.\n",
      "\n",
      "4. Fields for LLM:\n",
      "- La génération du contenu textuel des tweets pourrait bénéficier d'un modèle de langage pour assurer la cohérence et la pertinence.\n",
      "\n",
      "5. Sequential Nature:\n",
      "- Cette table a une nature séquentielle basée sur l'ordre chronologique des dates de création des comptes et des dates des tweets...Required Structure: {\n",
      "    \"twitter_api\": [\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u2387hj10\",\n",
      "                \"username\": \"user123\",\n",
      "                \"bio\": \"Lorem ipsum dolor sit amet\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1200,\n",
      "                \"following_count\": 600,\n",
      "                \"tweet_count\": 2300,\n",
      "                \"account_creation_date\": \"15/06/2015\",\n",
      "                \"tweet_id\": \"t4390dfg\",\n",
      "                \"tweet_text\": \"This is a sample tweet text\",\n",
      "                \"tweet_date\": \"20/08/2015 14:30:00\",\n",
      "                \"likes_count\": 110,\n",
      "                \"retweets_count\": 55,\n",
      "                \"replies_count\": 25\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u3985klmn\",\n",
      "                \"username\": \"abc_user\",\n",
      "                \"bio\": \"Consectetur adipiscing elit\",\n",
      "                \"location\": \"UK\",\n",
      "                \"followers_count\": 1500,\n",
      "                \"following_count\": 700,\n",
      "                \"tweet_count\": 2600,\n",
      "                \"account_creation_date\": \"25/03/2012\",\n",
      "                \"tweet_id\": \"t5678qwe\",\n",
      "                \"tweet_text\": \"Another sample tweet text\",\n",
      "                \"tweet_date\": \"10/10/2012 09:45:00\",\n",
      "                \"likes_count\": 90,\n",
      "                \"retweets_count\": 45,\n",
      "                \"replies_count\": 20\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u4567opqr\",\n",
      "                \"username\": \"test_user\",\n",
      "                \"bio\": \"Sed do eiusmod tempor incididunt\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1300,\n",
      "                \"following_count\": 550,\n",
      "                \"tweet_count\": 2100,\n",
      "                \"account_creation_date\": \"08/12/2018\",\n",
      "                \"tweet_id\": \"t6789zxc\",\n",
      "                \"tweet_text\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit\",\n",
      "                \"tweet_date\": \"05/05/2019 17:20:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 60,\n",
      "                \"replies_count\": 30\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}.The previous generated batch (maintain sequential patterns if needed): unknown.Add Randomness: 0.9533039250439691.It is critical that you don't repeat the same values every time you get this prompt.\n",
      "Generated Output:\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate 10 Sample synthetic items(records in a table or the equivalent elements in unstructured text) for the data described below. In the following structure, but with different values.For the following task: Data description:  ; The required region: France ; All texts should be translated to French language. ; The guidance given by an expert: 1. Relevant Columns:\n",
      "- Identifiant de l'utilisateur (identifiant unique)\n",
      "- Nom d'utilisateur (texte libre)\n",
      "- Bio (texte libre)\n",
      "- Lieu (catégorique)\n",
      "- Nombre d'abonnés (numérique)\n",
      "- Nombre de personnes suivies (numérique)\n",
      "- Nombre de tweets (numérique)\n",
      "- Date de création du compte (datetime)\n",
      "- Identifiant du tweet (identifiant unique)\n",
      "- Texte du tweet (texte libre)\n",
      "- Date du tweet (datetime)\n",
      "- Nombre de \"J'aime\" (numérique)\n",
      "- Nombre de retweets (numérique)\n",
      "- Nombre de réponses (numérique)\n",
      "\n",
      "2. Column Details:\n",
      "- Identifiant de l'utilisateur : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Nom d'utilisateur : longueur moyenne du texte - 7, écart-type de la longueur du texte - 2\n",
      "- Lieu : catégories - [États-Unis, Royaume-Uni, Canada, Australie, Autre], probabilités - [0.3, 0.2, 0.1, 0.1, 0.3]\n",
      "- Nombre d'abonnés : distribution - asymétrique positive, moyenne - 1000, écart-type - 500, min - 100, max - 5000\n",
      "- Nombre de personnes suivies : distribution - asymétrique positive, moyenne - 500, écart-type - 200, min - 50, max - 2000\n",
      "- Nombre de tweets : distribution - normalement distribuée, moyenne - 2000, écart-type - 1000, min - 500, max - 5000\n",
      "- Date de création du compte : format - jj/mm/aaaa, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 2 ans, écart-type - 1 an\n",
      "- Identifiant du tweet : format - alphanumérique, regEX - /^[0-9a-zA-Z]+$/\n",
      "- Texte du tweet : longueur moyenne du texte - 100, écart-type de la longueur du texte - 20\n",
      "- Date du tweet : format - jj/mm/aaaa hh:mm:ss, date minimale - 01/01/2010, date maximale - 31/12/2022, intervalles de temps moyens - 1 jour, écart-type - 6 heures\n",
      "- Nombre de \"J'aime\" : distribution - asymétrique positive, moyenne - 100, écart-type - 50, min - 10, max - 500\n",
      "- Nombre de retweets : distribution - asymétrique positive, moyenne - 50, écart-type - 20, min - 5, max - 200\n",
      "- Nombre de réponses : distribution - normalement distribuée, moyenne - 20, écart-type - 10, min - 1, max - 100\n",
      "\n",
      "3. Constraints and Dependencies:\n",
      "- L'identifiant de l'utilisateur est unique et doit être maintenu comme clé primaire.\n",
      "- L'identifiant du tweet est unique et doit être maintenu comme clé primaire.\n",
      "- Les compteurs d'engagements (likes, retweets, réponses) doivent dépendre du tweet correspondant.\n",
      "\n",
      "4. Fields for LLM:\n",
      "- La génération du contenu textuel des tweets pourrait bénéficier d'un modèle de langage pour assurer la cohérence et la pertinence.\n",
      "\n",
      "5. Sequential Nature:\n",
      "- Cette table a une nature séquentielle basée sur l'ordre chronologique des dates de création des comptes et des dates des tweets...Required Structure: {\n",
      "    \"twitter_api\": [\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u2387hj10\",\n",
      "                \"username\": \"user123\",\n",
      "                \"bio\": \"Lorem ipsum dolor sit amet\",\n",
      "                \"location\": \"USA\",\n",
      "                \"followers_count\": 1200,\n",
      "                \"following_count\": 600,\n",
      "                \"tweet_count\": 2300,\n",
      "                \"account_creation_date\": \"15/06/2015\",\n",
      "                \"tweet_id\": \"t4390dfg\",\n",
      "                \"tweet_text\": \"This is a sample tweet text\",\n",
      "                \"tweet_date\": \"20/08/2015 14:30:00\",\n",
      "                \"likes_count\": 110,\n",
      "                \"retweets_count\": 55,\n",
      "                \"replies_count\": 25\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u3985klmn\",\n",
      "                \"username\": \"abc_user\",\n",
      "                \"bio\": \"Consectetur adipiscing elit\",\n",
      "                \"location\": \"UK\",\n",
      "                \"followers_count\": 1500,\n",
      "                \"following_count\": 700,\n",
      "                \"tweet_count\": 2600,\n",
      "                \"account_creation_date\": \"25/03/2012\",\n",
      "                \"tweet_id\": \"t5678qwe\",\n",
      "                \"tweet_text\": \"Another sample tweet text\",\n",
      "                \"tweet_date\": \"10/10/2012 09:45:00\",\n",
      "                \"likes_count\": 90,\n",
      "                \"retweets_count\": 45,\n",
      "                \"replies_count\": 20\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"input\": {\n",
      "                \"user_id\": \"u4567opqr\",\n",
      "                \"username\": \"test_user\",\n",
      "                \"bio\": \"Sed do eiusmod tempor incididunt\",\n",
      "                \"location\": \"Canada\",\n",
      "                \"followers_count\": 1300,\n",
      "                \"following_count\": 550,\n",
      "                \"tweet_count\": 2100,\n",
      "                \"account_creation_date\": \"08/12/2018\",\n",
      "                \"tweet_id\": \"t6789zxc\",\n",
      "                \"tweet_text\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit\",\n",
      "                \"tweet_date\": \"05/05/2019 17:20:00\",\n",
      "                \"likes_count\": 120,\n",
      "                \"retweets_count\": 60,\n",
      "                \"replies_count\": 30\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"status\": \"success\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}.The previous generated batch (maintain sequential patterns if needed): unknown.Add Randomness: 0.7936704760923782.It is critical that you don't repeat the same values every time you get this prompt.\n",
      "Generated Output:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:30:22.259082Z",
     "start_time": "2024-05-02T08:30:22.248247Z"
    }
   },
   "source": [
    "generated_data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'twitter_api':    input.user_id    input.username  \\\n",
       " 0       u5je9h1a       superstar23   \n",
       " 1       u76tyk2h        voyageur78   \n",
       " 2       u9hj4k8n       fashionista   \n",
       " 3       u2p3o5i7      fitnesslover   \n",
       " 4       u6k8jd4h           gourmet   \n",
       " 5       u87fgh2a      photographie   \n",
       " 6       u3gh5kj9       cinemalover   \n",
       " 7       u4hjbr8l           artiste   \n",
       " 8       u543op0i       naturelover   \n",
       " 9       u2n34gf5   tech_enthusiast   \n",
       " 10      u2p48j9s          bookworm   \n",
       " 11     u5678ghj2           user456   \n",
       " 12     u7890abcd    utilisateur789   \n",
       " 13     u1234efg5           test123   \n",
       " 14     u2345ijk6      exemple_user   \n",
       " 15     u7890lmn1   utilisateur7890   \n",
       " 16     u3456opq7           test456   \n",
       " 17     u1234ijko  utilisateur_test   \n",
       " 18     u4567klm8         user_test   \n",
       " 19     u5678zxc9        exemple123   \n",
       " \n",
       "                                             input.bio input.location  \\\n",
       " 0              Bonjour! Je suis passionné de musique.          Autre   \n",
       " 1                 Passionné de voyages et d'aventure.    Royaume-Uni   \n",
       " 2                             La mode est ma passion!         France   \n",
       " 3                   Entraînement, santé et bien-être.     États-Unis   \n",
       " 4                Amateur de cuisine et de bons plats.      Australie   \n",
       " 5   Capturer des moments inoubliables à travers mo...         France   \n",
       " 6         Passionné de cinéma et de films classiques.          Autre   \n",
       " 7        L'art est ma façon d'exprimer ma créativité.          Autre   \n",
       " 8   Explorer la beauté de la nature et de l'enviro...      Australie   \n",
       " 9             Toujours à la pointe de la technologie.    Royaume-Uni   \n",
       " 10        Dévoreur de livres et passionné de lecture.         Canada   \n",
       " 11    Sed ut perspiciatis unde omnis iste natus error          Autre   \n",
       " 12  Nemo enim ipsam voluptatem quia voluptas sit a...      Australie   \n",
       " 13              Quis autem vel eum iure reprehenderit    Royaume-Uni   \n",
       " 14     Excepteur sint occaecat cupidatat non proident     États-Unis   \n",
       " 15  Duis aute irure dolor in reprehenderit in volu...          Autre   \n",
       " 16  Sunt in culpa qui officia deserunt mollit anim...          Autre   \n",
       " 17  Lorem ipsum dolor sit amet, consectetur adipis...          Autre   \n",
       " 18  Ut enim ad minim veniam, quis nostrud exercita...          Autre   \n",
       " 19  Exercitation ullamco laboris nisi ut aliquip e...          Autre   \n",
       " \n",
       "     input.followers_count  input.following_count  input.tweet_count  \\\n",
       " 0                     950                    520               1800   \n",
       " 1                    1350                    620               2450   \n",
       " 2                    1100                    540               2200   \n",
       " 3                    1250                    580               2300   \n",
       " 4                    1050                    530               2000   \n",
       " 5                    1150                    560               2100   \n",
       " 6                     970                    510               1900   \n",
       " 7                     990                    525               1950   \n",
       " 8                    1020                    540               2050   \n",
       " 9                    1300                    590               2150   \n",
       " 10                   1070                    535               1975   \n",
       " 11                    900                    400               1800   \n",
       " 12                   1100                    650               2400   \n",
       " 13                   1400                    700               2500   \n",
       " 14                    950                    450               1900   \n",
       " 15                   1050                    500               2000   \n",
       " 16                    960                    470               1950   \n",
       " 17                   1020                    480               1960   \n",
       " 18                    980                    460               1850   \n",
       " 19                    970                    455               1925   \n",
       " \n",
       "    input.account_creation_date input.tweet_id  \\\n",
       " 0                   21/07/2014       t3sd5f89   \n",
       " 1                   12/04/2011       t098fa3g   \n",
       " 2                   29/10/2016       t567g8j3   \n",
       " 3                   07/03/2013       t890df4g   \n",
       " 4                   14/08/2015       t456fgh9   \n",
       " 5                   25/06/2017       t234gh5j   \n",
       " 6                   18/12/2014       t543jg7s   \n",
       " 7                   02/09/2013       t678as2d   \n",
       " 8                   09/05/2016       t890kls2   \n",
       " 9                   30/11/2012       t45l7k98   \n",
       " 10                  16/01/2015       t678as90   \n",
       " 11                  12/09/2013       t2345hjk   \n",
       " 12                  29/11/2016       t3456vbn   \n",
       " 13                  03/07/2014       t4567yui   \n",
       " 14                  21/05/2015       t5678bnm   \n",
       " 15                  16/10/2017       t6789poi   \n",
       " 16                  09/02/2016       t7890zxc   \n",
       " 17                  24/08/2019       t8901vbn   \n",
       " 18                  02/04/2018       t9012dfg   \n",
       " 19                  06/01/2014       t0123rty   \n",
       " \n",
       "                                      input.tweet_text     input.tweet_date  \\\n",
       " 0                          Vive la musique française!  28/09/2014 11:15:00   \n",
       " 1           Découvrons de nouveaux horizons ensemble!  02/11/2011 08:50:00   \n",
       " 2   Les tendances de la mode cette saison sont inc...  15/02/2017 16:40:00   \n",
       " 3           Une vie saine commence par de bons choix!  19/09/2013 13:20:00   \n",
       " 4       Partageons des recettes délicieuses ensemble!  30/11/2015 10:10:00   \n",
       " 5           Chaque photo raconte une histoire unique.  03/12/2017 14:45:00   \n",
       " 6       Les chefs-d'œuvre du cinéma sont intemporels!  25/02/2015 09:30:00   \n",
       " 7               Chaque œuvre est une part de mon âme.  10/01/2014 12:20:00   \n",
       " 8   Protégeons notre planète pour les générations ...  22/10/2016 15:55:00   \n",
       " 9   Les innovations technologiques changent le monde!  12/05/2013 18:30:00   \n",
       " 10  Chaque livre est une porte vers un nouvel univ...  28/04/2015 11:40:00   \n",
       " 11                 Un autre exemple de texte de tweet  20/03/2014 10:15:00   \n",
       " 12          Encore un autre exemple de texte de tweet  18/02/2017 16:45:00   \n",
       " 13                 Voici un exemple de texte de tweet  12/12/2014 09:30:00   \n",
       " 14                          Un autre exemple de tweet  08/09/2015 14:00:00   \n",
       " 15              Ceci est un exemple de texte de tweet  25/01/2018 12:00:00   \n",
       " 16                          Un tweet de démonstration  28/04/2016 11:20:00   \n",
       " 17                   Encore un tweet de démonstration  03/12/2019 08:45:00   \n",
       " 18                       Juste un autre tweet de test  19/07/2018 15:10:00   \n",
       " 19                    Un dernier tweet pour l'exemple  14/04/2014 13:45:00   \n",
       " \n",
       "     input.likes_count  input.retweets_count  input.replies_count output.status  \n",
       " 0                  80                    40                   15       success  \n",
       " 1                  95                    50                   22       success  \n",
       " 2                 100                    55                   28       success  \n",
       " 3                  85                    48                   20       success  \n",
       " 4                  95                    45                   18       success  \n",
       " 5                 105                    52                   23       success  \n",
       " 6                  75                    42                   17       success  \n",
       " 7                  70                    38                   24       success  \n",
       " 8                  90                    46                   19       success  \n",
       " 9                 100                    48                   21       success  \n",
       " 10                 80                    44                   16       success  \n",
       " 11                 80                    40                   15       success  \n",
       " 12                100                    50                   20       success  \n",
       " 13                 95                    47                   23       success  \n",
       " 14                 85                    42                   19       success  \n",
       " 15                105                    52                   21       success  \n",
       " 16                 88                    44                   18       success  \n",
       " 17                 92                    46                   22       success  \n",
       " 18                 86                    43                   17       success  \n",
       " 19                 87                    41                   16       success  }"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Example Data to Data Structure and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide outlines the process of extracting a data structure from a given CSV file and using that structure to generate new data. This is particularly useful when you have sample data and want to replicate its structure with new, synthesized content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data Structure from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, you'll need to specify the path to your CSV file and the file name. Then, use the create_json_sample_from_csv function to extract the data structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:43:30.764108Z",
     "start_time": "2024-05-02T08:43:30.752728Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# Specify the path and file name\n",
    "path = r'C:\\Users\\Sigal\\data\\\\'\n",
    "file_name = r\"titanic.csv\"\n",
    "\n",
    "# Extract data structure from the CSV file\n",
    "dataStructureSample = create_json_sample_from_csv(path, file_name)\n",
    "print(dataStructureSample)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"titanic.csv\": [{\"PassengerId\":628,\"Survived\":1,\"Pclass\":1,\"Name\":\"Longley, Miss. Gretchen Fiske\",\"Sex\":\"female\",\"Age\":21.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"13502\",\"Fare\":77.9583,\"Cabin\":\"D9\",\"Embarked\":\"S\"},{\"PassengerId\":391,\"Survived\":1,\"Pclass\":1,\"Name\":\"Carter, Mr. William Ernest\",\"Sex\":\"male\",\"Age\":36.0,\"SibSp\":1,\"Parch\":2,\"Ticket\":\"113760\",\"Fare\":120.0,\"Cabin\":\"B96 B98\",\"Embarked\":\"S\"},{\"PassengerId\":215,\"Survived\":0,\"Pclass\":3,\"Name\":\"Kiernan, Mr. Philip\",\"Sex\":\"male\",\"Age\":null,\"SibSp\":1,\"Parch\":0,\"Ticket\":\"367229\",\"Fare\":7.75,\"Cabin\":null,\"Embarked\":\"Q\"},{\"PassengerId\":254,\"Survived\":0,\"Pclass\":3,\"Name\":\"Lobb, Mr. William Arthur\",\"Sex\":\"male\",\"Age\":30.0,\"SibSp\":1,\"Parch\":0,\"Ticket\":\"A\\/5. 3336\",\"Fare\":16.1,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":763,\"Survived\":1,\"Pclass\":3,\"Name\":\"Barah, Mr. Hanna Assi\",\"Sex\":\"male\",\"Age\":20.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"2663\",\"Fare\":7.2292,\"Cabin\":null,\"Embarked\":\"C\"}]}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Data Generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data structure extracted, you're ready to initialize the DataConsumer object that will be used for generating data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:43:34.894420Z",
     "start_time": "2024-05-02T08:43:34.887602Z"
    }
   },
   "source": [
    "# Construct the full file path\n",
    "full_file_path = os.path.join(path, file_name)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(full_file_path)\n",
    "\n",
    "# Initialize the DataConsumer with the extracted data structure\n",
    "DataAugmentorObj = DataAugmentor(llm=llm, structure=dataStructureSample)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Example Dataframe (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the generated data to be influenced by examples from your original dataset, you can set an examples DataFrame within your DataConsumer object.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:44:19.441925Z",
     "start_time": "2024-05-02T08:44:19.437854Z"
    }
   },
   "source": [
    "DataAugmentorObj.set_examples_dataframe(df, file_name)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Preview Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can generate and preview the synthesized data based on the extracted structure and any example data provided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:44:42.460331Z",
     "start_time": "2024-05-02T08:44:29.570777Z"
    }
   },
   "source": [
    "generated_data = DataAugmentorObj.preview_output_sample()\n",
    "print(generated_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in generating synthetic data according to user requests.Generate 10 Sample synthetic items(records in a table or the equivalent elements in unstructured text) for the data described below. In the following structure, but with different values.For the following task: Data description: None.Required Structure: {\"titanic.csv\": [{\"PassengerId\":381,\"Survived\":1,\"Pclass\":1,\"Name\":\"Bidois, Miss. Rosalie\",\"Sex\":\"female\",\"Age\":42.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"PC 17757\",\"Fare\":227.525,\"Cabin\":null,\"Embarked\":\"C\"},{\"PassengerId\":628,\"Survived\":1,\"Pclass\":1,\"Name\":\"Longley, Miss. Gretchen Fiske\",\"Sex\":\"female\",\"Age\":21.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"13502\",\"Fare\":77.9583,\"Cabin\":\"D9\",\"Embarked\":\"S\"},{\"PassengerId\":398,\"Survived\":0,\"Pclass\":2,\"Name\":\"McKane, Mr. Peter David\",\"Sex\":\"male\",\"Age\":46.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"28403\",\"Fare\":26.0,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":416,\"Survived\":0,\"Pclass\":3,\"Name\":\"Meek, Mrs. Thomas (Annie Louise Rowley)\",\"Sex\":\"female\",\"Age\":null,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"343095\",\"Fare\":8.05,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":31,\"Survived\":0,\"Pclass\":1,\"Name\":\"Uruchurtu, Don. Manuel E\",\"Sex\":\"male\",\"Age\":40.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"PC 17601\",\"Fare\":27.7208,\"Cabin\":null,\"Embarked\":\"C\"},{\"PassengerId\":465,\"Survived\":0,\"Pclass\":3,\"Name\":\"Maisner, Mr. Simon\",\"Sex\":\"male\",\"Age\":null,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"A\\/S 2816\",\"Fare\":8.05,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":561,\"Survived\":0,\"Pclass\":3,\"Name\":\"Morrow, Mr. Thomas Rowan\",\"Sex\":\"male\",\"Age\":null,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"372622\",\"Fare\":7.75,\"Cabin\":null,\"Embarked\":\"Q\"},{\"PassengerId\":796,\"Survived\":0,\"Pclass\":2,\"Name\":\"Otter, Mr. Richard\",\"Sex\":\"male\",\"Age\":39.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"28213\",\"Fare\":13.0,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":593,\"Survived\":0,\"Pclass\":3,\"Name\":\"Elsbury, Mr. William James\",\"Sex\":\"male\",\"Age\":47.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"A\\/5 3902\",\"Fare\":7.25,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":550,\"Survived\":1,\"Pclass\":2,\"Name\":\"Davies, Master. John Morgan Jr\",\"Sex\":\"male\",\"Age\":8.0,\"SibSp\":1,\"Parch\":1,\"Ticket\":\"C.A. 33112\",\"Fare\":36.75,\"Cabin\":null,\"Embarked\":\"S\"}]}.The previous generated batch (maintain sequential patterns if needed): {\"titanic.csv\": [{\"PassengerId\":395,\"Survived\":0,\"Pclass\":2,\"Name\":\"Karaic, Mr. Stefan\",\"Sex\":\"male\",\"Age\":45.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"349210\",\"Fare\":7.4958,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":119,\"Survived\":0,\"Pclass\":3,\"Name\":\"Farrell, Mr. James\",\"Sex\":\"male\",\"Age\":40.5,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"365235\",\"Fare\":7.75,\"Cabin\":null,\"Embarked\":\"Q\"},{\"PassengerId\":785,\"Survived\":0,\"Pclass\":3,\"Name\":\"Ali, Mr. William\",\"Sex\":\"male\",\"Age\":25.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"SOTON\\/O.Q. 3101312\",\"Fare\":7.05,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":693,\"Survived\":0,\"Pclass\":3,\"Name\":\"Lam, Mr. Ali\",\"Sex\":\"male\",\"Age\":31.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"1601\",\"Fare\":56.4958,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":355,\"Survived\":1,\"Pclass\":3,\"Name\":\"Yousif, Mr. Aziz\",\"Sex\":\"male\",\"Age\":null,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"A\\/5. 2151\",\"Fare\":8.05,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":510,\"Survived\":0,\"Pclass\":3,\"Name\":\"Lang, Mr. Fang\",\"Sex\":\"male\",\"Age\":26.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"1601\",\"Fare\":56.4958,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":600,\"Survived\":1,\"Pclass\":1,\"Name\":\"Duff Gordon, Sir. Cosmo Edmund\",\"Sex\":\"male\",\"Age\":49.0,\"SibSp\":1,\"Parch\":0,\"Ticket\":\"11755\",\"Fare\":39.6,\"Cabin\":\"A16\",\"Embarked\":\"C\"},{\"PassengerId\":882,\"Survived\":0,\"Pclass\":3,\"Name\":\"Markun, Mr. Johann\",\"Sex\":\"male\",\"Age\":33.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"349257\",\"Fare\":7.8958,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":432,\"Survived\":0,\"Pclass\":3,\"Name\":\"Thorneycroft, Mr. Percival\",\"Sex\":\"male\",\"Age\":null,\"SibSp\":1,\"Parch\":0,\"Ticket\":\"376564\",\"Fare\":16.1,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":201,\"Survived\":0,\"Pclass\":3,\"Name\":\"Vande Walle, Mr. Nestor Cyriel\",\"Sex\":\"male\",\"Age\":28.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"345770\",\"Fare\":9.5,\"Cabin\":null,\"Embarked\":\"S\"}]}.Add Randomness: 0.795124995779723.It is critical that you don't repeat the same values every time you get this prompt.\n",
      "Generated Output:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'titanic.csv':    PassengerId  Survived  Pclass  \\\n",
      "0          287         1       2   \n",
      "1          411         1       3   \n",
      "2          268         0       2   \n",
      "3          203         1       3   \n",
      "4          174         0       3   \n",
      "5          542         1       3   \n",
      "6          322         0       3   \n",
      "7          624         0       3   \n",
      "8          525         0       3   \n",
      "9           33         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                                Brown, Miss. Amelia  female  35.0      1   \n",
      "1                                  Sdycoff, Mr. Ivan    male  24.0      0   \n",
      "2                    Hamalainen, Mrs. William (Anna)  female  24.0      0   \n",
      "3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
      "4                          Sivola, Mr. Antti Wilhelm    male  21.0      0   \n",
      "5               Rice, Mrs. William (Margaret Norton)  female  39.0      0   \n",
      "6                                   Dans, Mr. Victor    male  22.0      0   \n",
      "7                            Hansen, Mr. Henrik Juul    male  26.0      1   \n",
      "8                                  Kassem, Mr. Fared    male  27.0      0   \n",
      "9                                  Mack, Mrs. (Mary)  female  50.0      0   \n",
      "\n",
      "   Parch             Ticket     Fare Cabin Embarked  \n",
      "0      2             237671  25.0000  None        S  \n",
      "1      0             349222   7.8958  None        S  \n",
      "2      2             250649  14.5000  None        S  \n",
      "3      2             347742  11.1333  None        S  \n",
      "4      0  STON/O 2. 3101285   7.9250  None        S  \n",
      "5      5             382652  29.1250  None        Q  \n",
      "6      0          A/5 21172   7.2500  None        S  \n",
      "7      0             350029   7.8542  None        S  \n",
      "8      0               2700   7.2292  None        C  \n",
      "9      0             370377   7.7500  None        Q  }\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Source Data with a textual description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the DataTransformer to apply transformations to the source data based on a given description. This could be used for data enrichment, data preparation and data explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates how to add a field indicating the main explanation for each passenger's survival in the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:33:00.814150Z",
     "start_time": "2024-05-02T08:33:00.807749Z"
    }
   },
   "source": [
    "from src.DataTransformer import DataTransformer  # Assuming the necessary import"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:33:00.818621Z",
     "start_time": "2024-05-02T08:33:00.815195Z"
    }
   },
   "source": [
    "# Instantiate the DataTransformer with the language model\n",
    "DataTransformerObj = DataTransformer(llm=llm)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:33:16.024032Z",
     "start_time": "2024-05-02T08:33:00.819659Z"
    }
   },
   "source": [
    "# Define the transformation based on a user's description and apply it to the data.\n",
    "query = \"Could you add a field indicating the main explanation for the survival of each passenger?\"\n",
    "results = DataTransformerObj.define_transformation(source_data={\"titanic\": df}, description=query)\n",
    "print(\"Transformed Sample Data:\")\n",
    "print(results)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SequentialChain chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\":879,\"Survived\":0,\"Pclass\":3,\"Name\":\"Laleff, Mr. Kristo\",\"Sex\":\"male\",\"Age\":null,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"349217\",\"Fare\":7.8958,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":131,\"Survived\":0,\"Pclass\":3,\"Name\":\"Drazenoic, Mr. Jozef\",\"Sex\":\"male\",\"Age\":33.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"349241\",\"Fare\":7.8958,\"Cabin\":null,\"Embarked\":\"C\"},{\"PassengerId\":795,\"Survived\":0,\"Pclass\":3,\"Name\":\"Dantcheff, Mr. Ristiu\",\"Sex\":\"male\",\"Age\":25.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"349203\",\"Fare\":7.8958,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":434,\"Survived\":0,\"Pclass\":3,\"Name\":\"Kallio, Mr. Nikolai Erland\",\"Sex\":\"male\",\"Age\":17.0,\"SibSp\":0,\"Parch\":0,\"Ticket\":\"STON\\/O 2. 3101274\",\"Fare\":7.125,\"Cabin\":null,\"Embarked\":\"S\"},{\"PassengerId\":803,\"Survived\":1,\"Pclass\":1,\"Name\":\"Carter, Master. William Thornton II\",\"Sex\":\"male\",\"Age\":11.0,\"SibSp\":1,\"Parch\":2,\"Ticket\":\"113760\",\"Fare\":120.0,\"Cabin\":\"B96 B98\",\"Embarked\":\"S\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Transformed Sample Data:\n",
      "{\n",
      "  \"titanic\": [\n",
      "    {\n",
      "      \"PassengerId\": 879,\n",
      "      \"Survived\": 0,\n",
      "      \"Pclass\": 3,\n",
      "      \"Name\": \"Laleff, Mr. Kristo\",\n",
      "      \"Sex\": \"male\",\n",
      "      \"Age\": null,\n",
      "      \"SibSp\": 0,\n",
      "      \"Parch\": 0,\n",
      "      \"Ticket\": \"349217\",\n",
      "      \"Fare\": 7.8958,\n",
      "      \"Cabin\": null,\n",
      "      \"Embarked\": \"S\",\n",
      "      \"Survival_Reason\": \"Low likelihood of survival based on class, male gender, and lack of family members on board\"\n",
      "    },\n",
      "    {\n",
      "      \"PassengerId\": 131,\n",
      "      \"Survived\": 0,\n",
      "      \"Pclass\": 3,\n",
      "      \"Name\": \"Drazenoic, Mr. Jozef\",\n",
      "      \"Sex\": \"male\",\n",
      "      \"Age\": 33.0,\n",
      "      \"SibSp\": 0,\n",
      "      \"Parch\": 0,\n",
      "      \"Ticket\": \"349241\",\n",
      "      \"Fare\": 7.8958,\n",
      "      \"Cabin\": null,\n",
      "      \"Embarked\": \"C\",\n",
      "      \"Survival_Reason\": \"Low likelihood of survival based on class, male gender, and age\"\n",
      "    },\n",
      "    {\n",
      "      \"PassengerId\": 795,\n",
      "      \"Survived\": 0,\n",
      "      \"Pclass\": 3,\n",
      "      \"Name\": \"Dantcheff, Mr. Ristiu\",\n",
      "      \"Sex\": \"male\",\n",
      "      \"Age\": 25.0,\n",
      "      \"SibSp\": 0,\n",
      "      \"Parch\": 0,\n",
      "      \"Ticket\": \"349203\",\n",
      "      \"Fare\": 7.8958,\n",
      "      \"Cabin\": null,\n",
      "      \"Embarked\": \"S\",\n",
      "      \"Survival_Reason\": \"Low likelihood of survival based on class, male gender, and age\"\n",
      "    },\n",
      "    {\n",
      "      \"PassengerId\": 434,\n",
      "      \"Survived\": 0,\n",
      "      \"Pclass\": 3,\n",
      "      \"Name\": \"Kallio, Mr. Nikolai Erland\",\n",
      "      \"Sex\": \"male\",\n",
      "      \"Age\": 17.0,\n",
      "      \"SibSp\": 0,\n",
      "      \"Parch\": 0,\n",
      "      \"Ticket\": \"STON/O 2. 3101274\",\n",
      "      \"Fare\": 7.125,\n",
      "      \"Cabin\": null,\n",
      "      \"Embarked\": \"S\",\n",
      "      \"Survival_Reason\": \"Low likelihood of survival based on class, male gender, and age\"\n",
      "    },\n",
      "    {\n",
      "      \"PassengerId\": 803,\n",
      "      \"Survived\": 1,\n",
      "      \"Pclass\": 1,\n",
      "      \"Name\": \"Carter, Master. William Thornton II\",\n",
      "      \"Sex\": \"male\",\n",
      "      \"Age\": 11.0,\n",
      "      \"SibSp\": 1,\n",
      "      \"Parch\": 2,\n",
      "      \"Ticket\": \"113760\",\n",
      "      \"Fare\": 120.0,\n",
      "      \"Cabin\": \"B96 B98\",\n",
      "      \"Embarked\": \"S\",\n",
      "      \"Survival_Reason\": \"Higher likelihood of survival based on class, age, male gender, and presence of family members on board\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Full Output"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:35:16.648249Z",
     "start_time": "2024-05-02T08:33:16.025112Z"
    }
   },
   "source": [
    "# Apply the transformation to the source data and print the results.\n",
    "results = DataTransformerObj.transform(source_data={\"titanic\": df[:50]})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 1, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Braund, Mr. Owen Harris\", \"Sex\": \"male\", \"Age\": 22.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"A/5 21171\", \"Fare\": 7.25, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 2, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Sex\": \"female\", \"Age\": 38.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"PC 17599\", \"Fare\": 71.2833, \"Cabin\": \"C85\", \"Embarked\": \"C\"}, {\"PassengerId\": 3, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Heikkinen, Miss. Laina\", \"Sex\": \"female\", \"Age\": 26.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"STON/O2. 3101282\", \"Fare\": 7.925, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 4, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Sex\": \"female\", \"Age\": 35.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"113803\", \"Fare\": 53.1, \"Cabin\": \"C123\", \"Embarked\": \"S\"}, {\"PassengerId\": 5, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Allen, Mr. William Henry\", \"Sex\": \"male\", \"Age\": 35.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"373450\", \"Fare\": 8.05, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 6, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Moran, Mr. James\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"330877\", \"Fare\": 8.4583, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 7, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"McCarthy, Mr. Timothy J\", \"Sex\": \"male\", \"Age\": 54.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"17463\", \"Fare\": 51.8625, \"Cabin\": \"E46\", \"Embarked\": \"S\"}, {\"PassengerId\": 8, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Palsson, Master. Gosta Leonard\", \"Sex\": \"male\", \"Age\": 2.0, \"SibSp\": 3, \"Parch\": 1, \"Ticket\": \"349909\", \"Fare\": 21.075, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 9, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\", \"Sex\": \"female\", \"Age\": 27.0, \"SibSp\": 0, \"Parch\": 2, \"Ticket\": \"347742\", \"Fare\": 11.1333, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 10, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Nasser, Mrs. Nicholas (Adele Achem)\", \"Sex\": \"female\", \"Age\": 14.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"237736\", \"Fare\": 30.0708, \"Cabin\": NaN, \"Embarked\": \"C\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 11, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Sandstrom, Miss. Marguerite Rut\", \"Sex\": \"female\", \"Age\": 4.0, \"SibSp\": 1, \"Parch\": 1, \"Ticket\": \"PP 9549\", \"Fare\": 16.7, \"Cabin\": \"G6\", \"Embarked\": \"S\"}, {\"PassengerId\": 12, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Bonnell, Miss. Elizabeth\", \"Sex\": \"female\", \"Age\": 58.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"113783\", \"Fare\": 26.55, \"Cabin\": \"C103\", \"Embarked\": \"S\"}, {\"PassengerId\": 13, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Saundercock, Mr. William Henry\", \"Sex\": \"male\", \"Age\": 20.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"A/5. 2151\", \"Fare\": 8.05, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 14, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Andersson, Mr. Anders Johan\", \"Sex\": \"male\", \"Age\": 39.0, \"SibSp\": 1, \"Parch\": 5, \"Ticket\": \"347082\", \"Fare\": 31.275, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 15, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Vestrom, Miss. Hulda Amanda Adolfina\", \"Sex\": \"female\", \"Age\": 14.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"350406\", \"Fare\": 7.8542, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 16, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Hewlett, Mrs. (Mary D Kingcome) \", \"Sex\": \"female\", \"Age\": 55.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"248706\", \"Fare\": 16.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 17, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Rice, Master. Eugene\", \"Sex\": \"male\", \"Age\": 2.0, \"SibSp\": 4, \"Parch\": 1, \"Ticket\": \"382652\", \"Fare\": 29.125, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 18, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Williams, Mr. Charles Eugene\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"244373\", \"Fare\": 13.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 19, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\", \"Sex\": \"female\", \"Age\": 31.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"345763\", \"Fare\": 18.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 20, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Masselmani, Mrs. Fatima\", \"Sex\": \"female\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"2649\", \"Fare\": 7.225, \"Cabin\": NaN, \"Embarked\": \"C\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 21, \"Survived\": 0, \"Pclass\": 2, \"Name\": \"Fynney, Mr. Joseph J\", \"Sex\": \"male\", \"Age\": 35.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"239865\", \"Fare\": 26.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 22, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Beesley, Mr. Lawrence\", \"Sex\": \"male\", \"Age\": 34.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"248698\", \"Fare\": 13.0, \"Cabin\": \"D56\", \"Embarked\": \"S\"}, {\"PassengerId\": 23, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"McGowan, Miss. Anna \\\\Annie\\\\\\\"\\\"\", \"Sex\": \"female\", \"Age\": 15.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"330923\", \"Fare\": 8.0292, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 24, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Sloper, Mr. William Thompson\", \"Sex\": \"male\", \"Age\": 28.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"113788\", \"Fare\": 35.5, \"Cabin\": \"A6\", \"Embarked\": \"S\"}, {\"PassengerId\": 25, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Palsson, Miss. Torborg Danira\", \"Sex\": \"female\", \"Age\": 8.0, \"SibSp\": 3, \"Parch\": 1, \"Ticket\": \"349909\", \"Fare\": 21.075, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 26, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)\", \"Sex\": \"female\", \"Age\": 38.0, \"SibSp\": 1, \"Parch\": 5, \"Ticket\": \"347077\", \"Fare\": 31.3875, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 27, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Emir, Mr. Farred Chehab\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"2631\", \"Fare\": 7.225, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 28, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"Fortune, Mr. Charles Alexander\", \"Sex\": \"male\", \"Age\": 19.0, \"SibSp\": 3, \"Parch\": 2, \"Ticket\": \"19950\", \"Fare\": 263.0, \"Cabin\": \"C23 C25 C27\", \"Embarked\": \"S\"}, {\"PassengerId\": 29, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"O'Dwyer, Miss. Ellen \\\\Nellie\\\\\\\"\\\"\", \"Sex\": \"female\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"330959\", \"Fare\": 7.8792, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 30, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Todoroff, Mr. Lalio\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"349216\", \"Fare\": 7.8958, \"Cabin\": NaN, \"Embarked\": \"S\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 31, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"Uruchurtu, Don. Manuel E\", \"Sex\": \"male\", \"Age\": 40.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"PC 17601\", \"Fare\": 27.7208, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 32, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Spencer, Mrs. William Augustus (Marie Eugenie)\", \"Sex\": \"female\", \"Age\": NaN, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"PC 17569\", \"Fare\": 146.5208, \"Cabin\": \"B78\", \"Embarked\": \"C\"}, {\"PassengerId\": 33, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Glynn, Miss. Mary Agatha\", \"Sex\": \"female\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"335677\", \"Fare\": 7.75, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 34, \"Survived\": 0, \"Pclass\": 2, \"Name\": \"Wheadon, Mr. Edward H\", \"Sex\": \"male\", \"Age\": 66.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"C.A. 24579\", \"Fare\": 10.5, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 35, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"Meyer, Mr. Edgar Joseph\", \"Sex\": \"male\", \"Age\": 28.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"PC 17604\", \"Fare\": 82.1708, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 36, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"Holverson, Mr. Alexander Oskar\", \"Sex\": \"male\", \"Age\": 42.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"113789\", \"Fare\": 52.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 37, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Mamee, Mr. Hanna\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"2677\", \"Fare\": 7.2292, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 38, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Cann, Mr. Ernest Charles\", \"Sex\": \"male\", \"Age\": 21.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"A./5. 2152\", \"Fare\": 8.05, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 39, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Vander Planke, Miss. Augusta Maria\", \"Sex\": \"female\", \"Age\": 18.0, \"SibSp\": 2, \"Parch\": 0, \"Ticket\": \"345764\", \"Fare\": 18.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 40, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Nicola-Yarred, Miss. Jamila\", \"Sex\": \"female\", \"Age\": 14.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"2651\", \"Fare\": 11.2417, \"Cabin\": NaN, \"Embarked\": \"C\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 41, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Ahlin, Mrs. Johan (Johanna Persdotter Larsson)\", \"Sex\": \"female\", \"Age\": 40.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"7546\", \"Fare\": 9.475, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 42, \"Survived\": 0, \"Pclass\": 2, \"Name\": \"Turpin, Mrs. William John Robert (Dorothy Ann Wonnacott)\", \"Sex\": \"female\", \"Age\": 27.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"11668\", \"Fare\": 21.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 43, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Kraeff, Mr. Theodor\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"349253\", \"Fare\": 7.8958, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 44, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Laroche, Miss. Simonne Marie Anne Andree\", \"Sex\": \"female\", \"Age\": 3.0, \"SibSp\": 1, \"Parch\": 2, \"Ticket\": \"SC/Paris 2123\", \"Fare\": 41.5792, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 45, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Devaney, Miss. Margaret Delia\", \"Sex\": \"female\", \"Age\": 19.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"330958\", \"Fare\": 7.8792, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 46, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Rogers, Mr. William John\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"S.C./A.4. 23567\", \"Fare\": 8.05, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 47, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Lennon, Mr. Denis\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"370371\", \"Fare\": 15.5, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 48, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"O'Driscoll, Miss. Bridget\", \"Sex\": \"female\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"14311\", \"Fare\": 7.75, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 49, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Samaan, Mr. Youssef\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 2, \"Parch\": 0, \"Ticket\": \"2662\", \"Fare\": 21.6792, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 50, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Arnold-Franchi, Mrs. Josef (Josefine Franchi)\", \"Sex\": \"female\", \"Age\": 18.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"349237\", \"Fare\": 17.8, \"Cabin\": NaN, \"Embarked\": \"S\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:35:16.671657Z",
     "start_time": "2024-05-02T08:35:16.650297Z"
    }
   },
   "source": [
    "pd.DataFrame(results['titanic'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    PassengerId  Survived  Pclass  \\\n",
       "0             1         0       3   \n",
       "1             2         1       1   \n",
       "2             3         1       3   \n",
       "3             4         1       1   \n",
       "4             5         0       3   \n",
       "5             6         0       3   \n",
       "6             7         0       1   \n",
       "7             8         0       3   \n",
       "8             9         1       3   \n",
       "9            10         1       2   \n",
       "10           11         1       3   \n",
       "11           12         1       1   \n",
       "12           13         0       3   \n",
       "13           14         0       3   \n",
       "14           15         0       3   \n",
       "15           16         1       2   \n",
       "16           17         0       3   \n",
       "17           18         1       2   \n",
       "18           19         0       3   \n",
       "19           20         1       3   \n",
       "20           21         0       2   \n",
       "21           22         1       2   \n",
       "22           23         1       3   \n",
       "23           24         1       1   \n",
       "24           25         0       3   \n",
       "25           26         1       3   \n",
       "26           27         0       3   \n",
       "27           28         0       1   \n",
       "28           29         1       3   \n",
       "29           30         0       3   \n",
       "30           31         0       1   \n",
       "31           32         1       1   \n",
       "32           33         1       3   \n",
       "33           34         0       2   \n",
       "34           35         0       1   \n",
       "35           36         0       1   \n",
       "36           37         1       3   \n",
       "37           38         0       3   \n",
       "38           39         0       3   \n",
       "39           40         1       3   \n",
       "40           41         0       3   \n",
       "41           42         0       2   \n",
       "42           43         0       3   \n",
       "43           44         1       2   \n",
       "44           45         1       3   \n",
       "45           46         0       3   \n",
       "46           47         0       3   \n",
       "47           48         1       3   \n",
       "48           49         0       3   \n",
       "49           50         0       3   \n",
       "\n",
       "                                                 Name     Sex   Age  SibSp  \\\n",
       "0                             Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1   Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                              Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3        Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                            Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                    Moran, Mr. James    male   NaN      0   \n",
       "6                             McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                      Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8   Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                 Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "10                    Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n",
       "11                           Bonnell, Miss. Elizabeth  female  58.0      0   \n",
       "12                     Saundercock, Mr. William Henry    male  20.0      0   \n",
       "13                        Andersson, Mr. Anders Johan    male  39.0      1   \n",
       "14               Vestrom, Miss. Hulda Amanda Adolfina  female  14.0      0   \n",
       "15                   Hewlett, Mrs. (Mary D Kingcome)   female  55.0      0   \n",
       "16                               Rice, Master. Eugene    male   2.0      4   \n",
       "17                       Williams, Mr. Charles Eugene    male   NaN      0   \n",
       "18  Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.0      1   \n",
       "19                            Masselmani, Mrs. Fatima  female   NaN      0   \n",
       "20                               Fynney, Mr. Joseph J    male  35.0      0   \n",
       "21                              Beesley, Mr. Lawrence    male  34.0      0   \n",
       "22                        McGowan, Miss. Anna \"Annie\"  female  15.0      0   \n",
       "23                       Sloper, Mr. William Thompson    male  28.0      0   \n",
       "24                      Palsson, Miss. Torborg Danira  female   8.0      3   \n",
       "25  Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.0      1   \n",
       "26                            Emir, Mr. Farred Chehab    male   NaN      0   \n",
       "27                     Fortune, Mr. Charles Alexander    male  19.0      3   \n",
       "28                      O'Dwyer, Miss. Ellen \"Nellie\"  female   NaN      0   \n",
       "29                                Todoroff, Mr. Lalio    male   NaN      0   \n",
       "30                           Uruchurtu, Don. Manuel E    male  40.0      0   \n",
       "31     Spencer, Mrs. William Augustus (Marie Eugenie)  female   NaN      1   \n",
       "32                           Glynn, Miss. Mary Agatha  female   NaN      0   \n",
       "33                              Wheadon, Mr. Edward H    male  66.0      0   \n",
       "34                            Meyer, Mr. Edgar Joseph    male  28.0      1   \n",
       "35                     Holverson, Mr. Alexander Oskar    male  42.0      1   \n",
       "36                                   Mamee, Mr. Hanna    male   NaN      0   \n",
       "37                           Cann, Mr. Ernest Charles    male  21.0      0   \n",
       "38                 Vander Planke, Miss. Augusta Maria  female  18.0      2   \n",
       "39                        Nicola-Yarred, Miss. Jamila  female  14.0      1   \n",
       "40     Ahlin, Mrs. Johan (Johanna Persdotter Larsson)  female  40.0      1   \n",
       "41  Turpin, Mrs. William John Robert (Dorothy Ann ...  female  27.0      1   \n",
       "42                                Kraeff, Mr. Theodor    male   NaN      0   \n",
       "43           Laroche, Miss. Simonne Marie Anne Andree  female   3.0      1   \n",
       "44                      Devaney, Miss. Margaret Delia  female  19.0      0   \n",
       "45                           Rogers, Mr. William John    male   NaN      0   \n",
       "46                                  Lennon, Mr. Denis    male   NaN      1   \n",
       "47                          O'Driscoll, Miss. Bridget  female   NaN      0   \n",
       "48                                Samaan, Mr. Youssef    male   NaN      2   \n",
       "49      Arnold-Franchi, Mrs. Josef (Josefine Franchi)  female  18.0      1   \n",
       "\n",
       "    Parch            Ticket      Fare        Cabin Embarked  \\\n",
       "0       0         A/5 21171    7.2500         None        S   \n",
       "1       0          PC 17599   71.2833          C85        C   \n",
       "2       0  STON/O2. 3101282    7.9250         None        S   \n",
       "3       0            113803   53.1000         C123        S   \n",
       "4       0            373450    8.0500         None        S   \n",
       "5       0            330877    8.4583         None        Q   \n",
       "6       0             17463   51.8625          E46        S   \n",
       "7       1            349909   21.0750         None        S   \n",
       "8       2            347742   11.1333         None        S   \n",
       "9       0            237736   30.0708         None        C   \n",
       "10      1           PP 9549   16.7000           G6        S   \n",
       "11      0            113783   26.5500         C103        S   \n",
       "12      0         A/5. 2151    8.0500          NaN        S   \n",
       "13      5            347082   31.2750          NaN        S   \n",
       "14      0            350406    7.8542          NaN        S   \n",
       "15      0            248706   16.0000          NaN        S   \n",
       "16      1            382652   29.1250          NaN        Q   \n",
       "17      0            244373   13.0000          NaN        S   \n",
       "18      0            345763   18.0000          NaN        S   \n",
       "19      0              2649    7.2250          NaN        C   \n",
       "20      0            239865   26.0000          NaN        S   \n",
       "21      0            248698   13.0000          D56        S   \n",
       "22      0            330923    8.0292          NaN        Q   \n",
       "23      0            113788   35.5000           A6        S   \n",
       "24      1            349909   21.0750          NaN        S   \n",
       "25      5            347077   31.3875          NaN        S   \n",
       "26      0              2631    7.2250          NaN        C   \n",
       "27      2             19950  263.0000  C23 C25 C27        S   \n",
       "28      0            330959    7.8792          NaN        Q   \n",
       "29      0            349216    7.8958          NaN        S   \n",
       "30      0          PC 17601   27.7208          NaN        C   \n",
       "31      0          PC 17569  146.5208          B78        C   \n",
       "32      0            335677    7.7500          NaN        Q   \n",
       "33      0        C.A. 24579   10.5000          NaN        S   \n",
       "34      0          PC 17604   82.1708          NaN        C   \n",
       "35      0            113789   52.0000          NaN        S   \n",
       "36      0              2677    7.2292          NaN        C   \n",
       "37      0        A./5. 2152    8.0500          NaN        S   \n",
       "38      0            345764   18.0000          NaN        S   \n",
       "39      0              2651   11.2417          NaN        C   \n",
       "40      0              7546    9.4750          NaN        S   \n",
       "41      0             11668   21.0000          NaN        S   \n",
       "42      0            349253    7.8958          NaN        C   \n",
       "43      2     SC/Paris 2123   41.5792          NaN        C   \n",
       "44      0            330958    7.8792          NaN        Q   \n",
       "45      0   S.C./A.4. 23567    8.0500          NaN        S   \n",
       "46      0            370371   15.5000          NaN        Q   \n",
       "47      0             14311    7.7500          NaN        Q   \n",
       "48      0              2662   21.6792          NaN        C   \n",
       "49      0            349237   17.8000          NaN        S   \n",
       "\n",
       "                                      Survival_Reason  \n",
       "0                                                 Age  \n",
       "1                                              Gender  \n",
       "2                                              Gender  \n",
       "3                                              Gender  \n",
       "4                                                 Age  \n",
       "5                                                 Age  \n",
       "6                                                 Age  \n",
       "7                                                 Age  \n",
       "8                                              Gender  \n",
       "9                                              Gender  \n",
       "10           Young age and presence of family members  \n",
       "11                             Upper class and gender  \n",
       "12                                               None  \n",
       "13                                               None  \n",
       "14                                          Young age  \n",
       "15                                     Gender and age  \n",
       "16           Young age and presence of family members  \n",
       "17                                               None  \n",
       "18                                               None  \n",
       "19                                               None  \n",
       "20  Low class male passengers had lower survival r...  \n",
       "21  Male passengers in higher classes had higher s...  \n",
       "22        Female passengers had higher survival rates  \n",
       "23  Male passengers in higher classes had higher s...  \n",
       "24  Presence of family members on board could affe...  \n",
       "25  Presence of family members on board could affe...  \n",
       "26               No specific explanation for survival  \n",
       "27  High fare paying passengers had higher surviva...  \n",
       "28               No specific explanation for survival  \n",
       "29               No specific explanation for survival  \n",
       "30  Low chance of survival due to being a male pas...  \n",
       "31  High chance of survival due to being a female ...  \n",
       "32  Medium chance of survival due to being a femal...  \n",
       "33  Low chance of survival due to being a male pas...  \n",
       "34  Low chance of survival due to being a male pas...  \n",
       "35  Low chance of survival due to being a male pas...  \n",
       "36  Medium chance of survival due to being a male ...  \n",
       "37  Low chance of survival due to being a male pas...  \n",
       "38  Low chance of survival due to being a female p...  \n",
       "39  High chance of survival due to being a female ...  \n",
       "40  Low chance of survival due to being a female a...  \n",
       "41  Low chance of survival due to being a female a...  \n",
       "42  Low chance of survival due to being a male adu...  \n",
       "43  High chance of survival due to being a female ...  \n",
       "44  High chance of survival due to being a female ...  \n",
       "45  Low chance of survival due to being a male adu...  \n",
       "46  Low chance of survival due to being a male adu...  \n",
       "47  High chance of survival due to being a female ...  \n",
       "48  Low chance of survival due to being a male adu...  \n",
       "49  Low chance of survival due to being a female a...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survival_Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>None</td>\n",
       "      <td>Q</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>None</td>\n",
       "      <td>C</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "      <td>Young age and presence of family members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C103</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper class and gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Young age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hewlett, Mrs. (Mary D Kingcome)</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248706</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Gender and age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Young age and presence of family members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Mrs. Julius (Emelia Maria Vande...</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>345763</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Fynney, Mr. Joseph J</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239865</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low class male passengers had lower survival r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beesley, Mr. Lawrence</td>\n",
       "      <td>male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248698</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>D56</td>\n",
       "      <td>S</td>\n",
       "      <td>Male passengers in higher classes had higher s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>McGowan, Miss. Anna \"Annie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330923</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Female passengers had higher survival rates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sloper, Mr. William Thompson</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113788</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>A6</td>\n",
       "      <td>S</td>\n",
       "      <td>Male passengers in higher classes had higher s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Miss. Torborg Danira</td>\n",
       "      <td>female</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Presence of family members on board could affe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347077</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Presence of family members on board could affe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2631</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>No specific explanation for survival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "      <td>High fare paying passengers had higher surviva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330959</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>No specific explanation for survival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Todoroff, Mr. Lalio</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349216</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>No specific explanation for survival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Uruchurtu, Don. Manuel E</td>\n",
       "      <td>male</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17601</td>\n",
       "      <td>27.7208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Low chance of survival due to being a male pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Spencer, Mrs. William Augustus (Marie Eugenie)</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17569</td>\n",
       "      <td>146.5208</td>\n",
       "      <td>B78</td>\n",
       "      <td>C</td>\n",
       "      <td>High chance of survival due to being a female ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Glynn, Miss. Mary Agatha</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>335677</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Medium chance of survival due to being a femal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Wheadon, Mr. Edward H</td>\n",
       "      <td>male</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A. 24579</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a male pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Meyer, Mr. Edgar Joseph</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17604</td>\n",
       "      <td>82.1708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Low chance of survival due to being a male pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Holverson, Mr. Alexander Oskar</td>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113789</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a male pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Mamee, Mr. Hanna</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2677</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Medium chance of survival due to being a male ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Cann, Mr. Ernest Charles</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A./5. 2152</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a male pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Miss. Augusta Maria</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>345764</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a female p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Nicola-Yarred, Miss. Jamila</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2651</td>\n",
       "      <td>11.2417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>High chance of survival due to being a female ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Ahlin, Mrs. Johan (Johanna Persdotter Larsson)</td>\n",
       "      <td>female</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7546</td>\n",
       "      <td>9.4750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a female a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Turpin, Mrs. William John Robert (Dorothy Ann ...</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11668</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a female a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kraeff, Mr. Theodor</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349253</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Low chance of survival due to being a male adu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Laroche, Miss. Simonne Marie Anne Andree</td>\n",
       "      <td>female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>SC/Paris 2123</td>\n",
       "      <td>41.5792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>High chance of survival due to being a female ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Devaney, Miss. Margaret Delia</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330958</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>High chance of survival due to being a female ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rogers, Mr. William John</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S.C./A.4. 23567</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a male adu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Lennon, Mr. Denis</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>370371</td>\n",
       "      <td>15.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Low chance of survival due to being a male adu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Driscoll, Miss. Bridget</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14311</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>High chance of survival due to being a female ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Samaan, Mr. Youssef</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2662</td>\n",
       "      <td>21.6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Low chance of survival due to being a male adu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Arnold-Franchi, Mrs. Josef (Josefine Franchi)</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>349237</td>\n",
       "      <td>17.8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a female a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Full Output in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Parallel Execution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:35:16.675424Z",
     "start_time": "2024-05-02T08:35:16.672668Z"
    }
   },
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Full Output in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the transformation on a subset of the Titanic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heree, the first 45 records of the Titanic dataset are processed in parallel using the asynchronous method.# The output is then collected and converted into a pandas DataFrame for easy viewing and further analysi.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:35:43.328999Z",
     "start_time": "2024-05-02T08:35:16.676434Z"
    }
   },
   "source": [
    "generated_data = asyncio.run(DataTransformerObj.transform_in_parallel(source_data={\"titanic\":df[:45]}))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 11, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Sandstrom, Miss. Marguerite Rut\", \"Sex\": \"female\", \"Age\": 4.0, \"SibSp\": 1, \"Parch\": 1, \"Ticket\": \"PP 9549\", \"Fare\": 16.7, \"Cabin\": \"G6\", \"Embarked\": \"S\"}, {\"PassengerId\": 12, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Bonnell, Miss. Elizabeth\", \"Sex\": \"female\", \"Age\": 58.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"113783\", \"Fare\": 26.55, \"Cabin\": \"C103\", \"Embarked\": \"S\"}, {\"PassengerId\": 13, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Saundercock, Mr. William Henry\", \"Sex\": \"male\", \"Age\": 20.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"A/5. 2151\", \"Fare\": 8.05, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 14, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Andersson, Mr. Anders Johan\", \"Sex\": \"male\", \"Age\": 39.0, \"SibSp\": 1, \"Parch\": 5, \"Ticket\": \"347082\", \"Fare\": 31.275, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 15, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Vestrom, Miss. Hulda Amanda Adolfina\", \"Sex\": \"female\", \"Age\": 14.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"350406\", \"Fare\": 7.8542, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 16, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Hewlett, Mrs. (Mary D Kingcome) \", \"Sex\": \"female\", \"Age\": 55.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"248706\", \"Fare\": 16.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 17, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Rice, Master. Eugene\", \"Sex\": \"male\", \"Age\": 2.0, \"SibSp\": 4, \"Parch\": 1, \"Ticket\": \"382652\", \"Fare\": 29.125, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 18, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Williams, Mr. Charles Eugene\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"244373\", \"Fare\": 13.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 19, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\", \"Sex\": \"female\", \"Age\": 31.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"345763\", \"Fare\": 18.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 20, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Masselmani, Mrs. Fatima\", \"Sex\": \"female\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"2649\", \"Fare\": 7.225, \"Cabin\": NaN, \"Embarked\": \"C\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 21, \"Survived\": 0, \"Pclass\": 2, \"Name\": \"Fynney, Mr. Joseph J\", \"Sex\": \"male\", \"Age\": 35.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"239865\", \"Fare\": 26.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 22, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Beesley, Mr. Lawrence\", \"Sex\": \"male\", \"Age\": 34.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"248698\", \"Fare\": 13.0, \"Cabin\": \"D56\", \"Embarked\": \"S\"}, {\"PassengerId\": 23, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"McGowan, Miss. Anna \\\\Annie\\\\\\\"\\\"\", \"Sex\": \"female\", \"Age\": 15.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"330923\", \"Fare\": 8.0292, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 24, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Sloper, Mr. William Thompson\", \"Sex\": \"male\", \"Age\": 28.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"113788\", \"Fare\": 35.5, \"Cabin\": \"A6\", \"Embarked\": \"S\"}, {\"PassengerId\": 25, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Palsson, Miss. Torborg Danira\", \"Sex\": \"female\", \"Age\": 8.0, \"SibSp\": 3, \"Parch\": 1, \"Ticket\": \"349909\", \"Fare\": 21.075, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 26, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)\", \"Sex\": \"female\", \"Age\": 38.0, \"SibSp\": 1, \"Parch\": 5, \"Ticket\": \"347077\", \"Fare\": 31.3875, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 27, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Emir, Mr. Farred Chehab\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"2631\", \"Fare\": 7.225, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 28, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"Fortune, Mr. Charles Alexander\", \"Sex\": \"male\", \"Age\": 19.0, \"SibSp\": 3, \"Parch\": 2, \"Ticket\": \"19950\", \"Fare\": 263.0, \"Cabin\": \"C23 C25 C27\", \"Embarked\": \"S\"}, {\"PassengerId\": 29, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"O'Dwyer, Miss. Ellen \\\\Nellie\\\\\\\"\\\"\", \"Sex\": \"female\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"330959\", \"Fare\": 7.8792, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 30, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Todoroff, Mr. Lalio\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"349216\", \"Fare\": 7.8958, \"Cabin\": NaN, \"Embarked\": \"S\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 1, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Braund, Mr. Owen Harris\", \"Sex\": \"male\", \"Age\": 22.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"A/5 21171\", \"Fare\": 7.25, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 2, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\", \"Sex\": \"female\", \"Age\": 38.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"PC 17599\", \"Fare\": 71.2833, \"Cabin\": \"C85\", \"Embarked\": \"C\"}, {\"PassengerId\": 3, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Heikkinen, Miss. Laina\", \"Sex\": \"female\", \"Age\": 26.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"STON/O2. 3101282\", \"Fare\": 7.925, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 4, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\", \"Sex\": \"female\", \"Age\": 35.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"113803\", \"Fare\": 53.1, \"Cabin\": \"C123\", \"Embarked\": \"S\"}, {\"PassengerId\": 5, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Allen, Mr. William Henry\", \"Sex\": \"male\", \"Age\": 35.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"373450\", \"Fare\": 8.05, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 6, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Moran, Mr. James\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"330877\", \"Fare\": 8.4583, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 7, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"McCarthy, Mr. Timothy J\", \"Sex\": \"male\", \"Age\": 54.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"17463\", \"Fare\": 51.8625, \"Cabin\": \"E46\", \"Embarked\": \"S\"}, {\"PassengerId\": 8, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Palsson, Master. Gosta Leonard\", \"Sex\": \"male\", \"Age\": 2.0, \"SibSp\": 3, \"Parch\": 1, \"Ticket\": \"349909\", \"Fare\": 21.075, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 9, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\", \"Sex\": \"female\", \"Age\": 27.0, \"SibSp\": 0, \"Parch\": 2, \"Ticket\": \"347742\", \"Fare\": 11.1333, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 10, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Nasser, Mrs. Nicholas (Adele Achem)\", \"Sex\": \"female\", \"Age\": 14.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"237736\", \"Fare\": 30.0708, \"Cabin\": NaN, \"Embarked\": \"C\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 31, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"Uruchurtu, Don. Manuel E\", \"Sex\": \"male\", \"Age\": 40.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"PC 17601\", \"Fare\": 27.7208, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 32, \"Survived\": 1, \"Pclass\": 1, \"Name\": \"Spencer, Mrs. William Augustus (Marie Eugenie)\", \"Sex\": \"female\", \"Age\": NaN, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"PC 17569\", \"Fare\": 146.5208, \"Cabin\": \"B78\", \"Embarked\": \"C\"}, {\"PassengerId\": 33, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Glynn, Miss. Mary Agatha\", \"Sex\": \"female\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"335677\", \"Fare\": 7.75, \"Cabin\": NaN, \"Embarked\": \"Q\"}, {\"PassengerId\": 34, \"Survived\": 0, \"Pclass\": 2, \"Name\": \"Wheadon, Mr. Edward H\", \"Sex\": \"male\", \"Age\": 66.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"C.A. 24579\", \"Fare\": 10.5, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 35, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"Meyer, Mr. Edgar Joseph\", \"Sex\": \"male\", \"Age\": 28.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"PC 17604\", \"Fare\": 82.1708, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 36, \"Survived\": 0, \"Pclass\": 1, \"Name\": \"Holverson, Mr. Alexander Oskar\", \"Sex\": \"male\", \"Age\": 42.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"113789\", \"Fare\": 52.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 37, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Mamee, Mr. Hanna\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"2677\", \"Fare\": 7.2292, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 38, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Cann, Mr. Ernest Charles\", \"Sex\": \"male\", \"Age\": 21.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"A./5. 2152\", \"Fare\": 8.05, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 39, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Vander Planke, Miss. Augusta Maria\", \"Sex\": \"female\", \"Age\": 18.0, \"SibSp\": 2, \"Parch\": 0, \"Ticket\": \"345764\", \"Fare\": 18.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 40, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Nicola-Yarred, Miss. Jamila\", \"Sex\": \"female\", \"Age\": 14.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"2651\", \"Fare\": 11.2417, \"Cabin\": NaN, \"Embarked\": \"C\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mYou are a system that specializes in enriching or transforming given source data with additional attributes according to user requests. Follow the given transformation logic for adding additional features. The task as described by the user: Could you add a field indicating the main explanation for the survival of each passenger?; The transformation logic: 1. Analyze the key factors that could explain the survival of passengers, such as their age, gender, class, and presence of family members on board.\n",
      "2. Assign weight or importance to each factor based on historical data or common knowledge about survival rates in similar situations.\n",
      "3. Use a decision-making process or algorithm to combine these factors and determine the main explanation for the survival of each passenger.\n",
      "4. Create a new field named \"Survival_Reason\" to store the main explanation for survival for each passenger.\n",
      "5. Populate the \"Survival_Reason\" field with the determined explanation for each passenger based on the analysis of key factors.; The output should be formatted as the input sample. The given source data is: {\"titanic\": [{\"PassengerId\": 41, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Ahlin, Mrs. Johan (Johanna Persdotter Larsson)\", \"Sex\": \"female\", \"Age\": 40.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"7546\", \"Fare\": 9.475, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 42, \"Survived\": 0, \"Pclass\": 2, \"Name\": \"Turpin, Mrs. William John Robert (Dorothy Ann Wonnacott)\", \"Sex\": \"female\", \"Age\": 27.0, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"11668\", \"Fare\": 21.0, \"Cabin\": NaN, \"Embarked\": \"S\"}, {\"PassengerId\": 43, \"Survived\": 0, \"Pclass\": 3, \"Name\": \"Kraeff, Mr. Theodor\", \"Sex\": \"male\", \"Age\": NaN, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"349253\", \"Fare\": 7.8958, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 44, \"Survived\": 1, \"Pclass\": 2, \"Name\": \"Laroche, Miss. Simonne Marie Anne Andree\", \"Sex\": \"female\", \"Age\": 3.0, \"SibSp\": 1, \"Parch\": 2, \"Ticket\": \"SC/Paris 2123\", \"Fare\": 41.5792, \"Cabin\": NaN, \"Embarked\": \"C\"}, {\"PassengerId\": 45, \"Survived\": 1, \"Pclass\": 3, \"Name\": \"Devaney, Miss. Margaret Delia\", \"Sex\": \"female\", \"Age\": 19.0, \"SibSp\": 0, \"Parch\": 0, \"Ticket\": \"330958\", \"Fare\": 7.8792, \"Cabin\": NaN, \"Embarked\": \"Q\"}]}; Please make sure you output a valid JSON format, and don't cut it in the middle. Please make sure the output only contains the dataset structure. Omit any text before or after the JSON structure.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the transformed data: \n",
    " The result of the asynchronous transformation is a dictionary with keys corresponding to dataset names and values as lists of dictionaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert this into a DataFrame for better readability and to leverage DataFrame functionalities for any subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T08:35:43.350452Z",
     "start_time": "2024-05-02T08:35:43.334273Z"
    }
   },
   "source": [
    "pd.DataFrame(generated_data['titanic'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    PassengerId  Survived  Pclass  \\\n",
       "0             1         0       3   \n",
       "1             2         1       1   \n",
       "2             3         1       3   \n",
       "3             4         1       1   \n",
       "4             5         0       3   \n",
       "5             6         0       3   \n",
       "6             7         0       1   \n",
       "7             8         0       3   \n",
       "8             9         1       3   \n",
       "9            10         1       2   \n",
       "10           11         1       3   \n",
       "11           12         1       1   \n",
       "12           13         0       3   \n",
       "13           14         0       3   \n",
       "14           15         0       3   \n",
       "15           16         1       2   \n",
       "16           17         0       3   \n",
       "17           18         1       2   \n",
       "18           19         0       3   \n",
       "19           20         1       3   \n",
       "20           21         0       2   \n",
       "21           22         1       2   \n",
       "22           23         1       3   \n",
       "23           24         1       1   \n",
       "24           25         0       3   \n",
       "25           26         1       3   \n",
       "26           27         0       3   \n",
       "27           28         0       1   \n",
       "28           29         1       3   \n",
       "29           30         0       3   \n",
       "30           31         0       1   \n",
       "31           32         1       1   \n",
       "32           33         1       3   \n",
       "33           34         0       2   \n",
       "34           35         0       1   \n",
       "35           36         0       1   \n",
       "36           37         1       3   \n",
       "37           38         0       3   \n",
       "38           39         0       3   \n",
       "39           40         1       3   \n",
       "40           41         0       3   \n",
       "41           42         0       2   \n",
       "42           43         0       3   \n",
       "43           44         1       2   \n",
       "44           45         1       3   \n",
       "\n",
       "                                                 Name     Sex   Age  SibSp  \\\n",
       "0                             Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1   Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                              Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3        Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                            Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                    Moran, Mr. James    male   NaN      0   \n",
       "6                             McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                      Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8   Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                 Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "10                    Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n",
       "11                           Bonnell, Miss. Elizabeth  female  58.0      0   \n",
       "12                     Saundercock, Mr. William Henry    male  20.0      0   \n",
       "13                        Andersson, Mr. Anders Johan    male  39.0      1   \n",
       "14               Vestrom, Miss. Hulda Amanda Adolfina  female  14.0      0   \n",
       "15                   Hewlett, Mrs. (Mary D Kingcome)   female  55.0      0   \n",
       "16                               Rice, Master. Eugene    male   2.0      4   \n",
       "17                       Williams, Mr. Charles Eugene    male   NaN      0   \n",
       "18  Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.0      1   \n",
       "19                            Masselmani, Mrs. Fatima  female   NaN      0   \n",
       "20                               Fynney, Mr. Joseph J    male  35.0      0   \n",
       "21                              Beesley, Mr. Lawrence    male  34.0      0   \n",
       "22                        McGowan, Miss. Anna \"Annie\"  female  15.0      0   \n",
       "23                       Sloper, Mr. William Thompson    male  28.0      0   \n",
       "24                      Palsson, Miss. Torborg Danira  female   8.0      3   \n",
       "25  Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.0      1   \n",
       "26                            Emir, Mr. Farred Chehab    male   NaN      0   \n",
       "27                     Fortune, Mr. Charles Alexander    male  19.0      3   \n",
       "28                      O'Dwyer, Miss. Ellen \"Nellie\"  female   NaN      0   \n",
       "29                                Todoroff, Mr. Lalio    male   NaN      0   \n",
       "30                           Uruchurtu, Don. Manuel E    male  40.0      0   \n",
       "31     Spencer, Mrs. William Augustus (Marie Eugenie)  female   NaN      1   \n",
       "32                           Glynn, Miss. Mary Agatha  female   NaN      0   \n",
       "33                              Wheadon, Mr. Edward H    male  66.0      0   \n",
       "34                            Meyer, Mr. Edgar Joseph    male  28.0      1   \n",
       "35                     Holverson, Mr. Alexander Oskar    male  42.0      1   \n",
       "36                                   Mamee, Mr. Hanna    male   NaN      0   \n",
       "37                           Cann, Mr. Ernest Charles    male  21.0      0   \n",
       "38                 Vander Planke, Miss. Augusta Maria  female  18.0      2   \n",
       "39                        Nicola-Yarred, Miss. Jamila  female  14.0      1   \n",
       "40     Ahlin, Mrs. Johan (Johanna Persdotter Larsson)  female  40.0      1   \n",
       "41  Turpin, Mrs. William John Robert (Dorothy Ann ...  female  27.0      1   \n",
       "42                                Kraeff, Mr. Theodor    male   NaN      0   \n",
       "43           Laroche, Miss. Simonne Marie Anne Andree  female   3.0      1   \n",
       "44                      Devaney, Miss. Margaret Delia  female  19.0      0   \n",
       "\n",
       "    Parch            Ticket      Fare        Cabin Embarked  \\\n",
       "0       0         A/5 21171    7.2500          NaN        S   \n",
       "1       0          PC 17599   71.2833          C85        C   \n",
       "2       0  STON/O2. 3101282    7.9250          NaN        S   \n",
       "3       0            113803   53.1000         C123        S   \n",
       "4       0            373450    8.0500          NaN        S   \n",
       "5       0            330877    8.4583          NaN        Q   \n",
       "6       0             17463   51.8625          E46        S   \n",
       "7       1            349909   21.0750          NaN        S   \n",
       "8       2            347742   11.1333          NaN        S   \n",
       "9       0            237736   30.0708          NaN        C   \n",
       "10      1           PP 9549   16.7000           G6        S   \n",
       "11      0            113783   26.5500         C103        S   \n",
       "12      0         A/5. 2151    8.0500         None        S   \n",
       "13      5            347082   31.2750         None        S   \n",
       "14      0            350406    7.8542         None        S   \n",
       "15      0            248706   16.0000         None        S   \n",
       "16      1            382652   29.1250         None        Q   \n",
       "17      0            244373   13.0000         None        S   \n",
       "18      0            345763   18.0000         None        S   \n",
       "19      0              2649    7.2250         None        C   \n",
       "20      0            239865   26.0000          NaN        S   \n",
       "21      0            248698   13.0000          D56        S   \n",
       "22      0            330923    8.0292          NaN        Q   \n",
       "23      0            113788   35.5000           A6        S   \n",
       "24      1            349909   21.0750          NaN        S   \n",
       "25      5            347077   31.3875          NaN        S   \n",
       "26      0              2631    7.2250          NaN        C   \n",
       "27      2             19950  263.0000  C23 C25 C27        S   \n",
       "28      0            330959    7.8792          NaN        Q   \n",
       "29      0            349216    7.8958          NaN        S   \n",
       "30      0          PC 17601   27.7208         None        C   \n",
       "31      0          PC 17569  146.5208          B78        C   \n",
       "32      0            335677    7.7500         None        Q   \n",
       "33      0        C.A. 24579   10.5000         None        S   \n",
       "34      0          PC 17604   82.1708         None        C   \n",
       "35      0            113789   52.0000         None        S   \n",
       "36      0              2677    7.2292         None        C   \n",
       "37      0        A./5. 2152    8.0500         None        S   \n",
       "38      0            345764   18.0000         None        S   \n",
       "39      0              2651   11.2417         None        C   \n",
       "40      0              7546    9.4750          NaN        S   \n",
       "41      0             11668   21.0000          NaN        S   \n",
       "42      0            349253    7.8958          NaN        C   \n",
       "43      2     SC/Paris 2123   41.5792          NaN        C   \n",
       "44      0            330958    7.8792          NaN        Q   \n",
       "\n",
       "                                      Survival_Reason  \n",
       "0                                         Age, Gender  \n",
       "1                          Age, Gender, Class, Family  \n",
       "2                                         Age, Gender  \n",
       "3                          Age, Gender, Class, Family  \n",
       "4                                         Age, Gender  \n",
       "5                                              Gender  \n",
       "6                                         Age, Gender  \n",
       "7                                 Age, Gender, Family  \n",
       "8                                 Age, Gender, Family  \n",
       "9                          Age, Gender, Class, Family  \n",
       "10           Young age and presence of family members  \n",
       "11                            Higher class and gender  \n",
       "12                                             Gender  \n",
       "13                         Presence of family members  \n",
       "14                                          Young age  \n",
       "15                            Higher class and gender  \n",
       "16           Young age and presence of family members  \n",
       "17                                             Gender  \n",
       "18                         Presence of family members  \n",
       "19                                             Gender  \n",
       "20                             Low class, male gender  \n",
       "21                                        Male gender  \n",
       "22                                      Female gender  \n",
       "23                                        Male gender  \n",
       "24       Low class, female gender, presence of family  \n",
       "25                  Female gender, presence of family  \n",
       "26                                            Unknown  \n",
       "27                    Male gender, presence of family  \n",
       "28                                            Unknown  \n",
       "29                                            Unknown  \n",
       "30                    Middle-aged male in first class  \n",
       "31                              Female in first class  \n",
       "32                              Female in third class  \n",
       "33                       Elderly male in second class  \n",
       "34                          Young male in first class  \n",
       "35                    Middle-aged male in first class  \n",
       "36                       Male survivor in third class  \n",
       "37                          Young male in third class  \n",
       "38          Female in third class with family members  \n",
       "39                        Female child in third class  \n",
       "40  Low chance of survival due to being a female i...  \n",
       "41  Low chance of survival due to being a female i...  \n",
       "42  Low chance of survival due to being a male in ...  \n",
       "43  High chance of survival due to being a female ...  \n",
       "44  High chance of survival due to being a female ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survival_Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Age, Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>Age, Gender, Class, Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Age, Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>Age, Gender, Class, Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Age, Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>Age, Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Age, Gender, Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Age, Gender, Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Age, Gender, Class, Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "      <td>Young age and presence of family members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C103</td>\n",
       "      <td>S</td>\n",
       "      <td>Higher class and gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Presence of family members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Young age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hewlett, Mrs. (Mary D Kingcome)</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248706</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Higher class and gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>None</td>\n",
       "      <td>Q</td>\n",
       "      <td>Young age and presence of family members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Mrs. Julius (Emelia Maria Vande...</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>345763</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Presence of family members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>None</td>\n",
       "      <td>C</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Fynney, Mr. Joseph J</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239865</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low class, male gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beesley, Mr. Lawrence</td>\n",
       "      <td>male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248698</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>D56</td>\n",
       "      <td>S</td>\n",
       "      <td>Male gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>McGowan, Miss. Anna \"Annie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330923</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Female gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sloper, Mr. William Thompson</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113788</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>A6</td>\n",
       "      <td>S</td>\n",
       "      <td>Male gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Miss. Torborg Danira</td>\n",
       "      <td>female</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low class, female gender, presence of family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347077</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Female gender, presence of family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2631</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "      <td>Male gender, presence of family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330959</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Todoroff, Mr. Lalio</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349216</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Uruchurtu, Don. Manuel E</td>\n",
       "      <td>male</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17601</td>\n",
       "      <td>27.7208</td>\n",
       "      <td>None</td>\n",
       "      <td>C</td>\n",
       "      <td>Middle-aged male in first class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Spencer, Mrs. William Augustus (Marie Eugenie)</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17569</td>\n",
       "      <td>146.5208</td>\n",
       "      <td>B78</td>\n",
       "      <td>C</td>\n",
       "      <td>Female in first class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Glynn, Miss. Mary Agatha</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>335677</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>None</td>\n",
       "      <td>Q</td>\n",
       "      <td>Female in third class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Wheadon, Mr. Edward H</td>\n",
       "      <td>male</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A. 24579</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Elderly male in second class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Meyer, Mr. Edgar Joseph</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17604</td>\n",
       "      <td>82.1708</td>\n",
       "      <td>None</td>\n",
       "      <td>C</td>\n",
       "      <td>Young male in first class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Holverson, Mr. Alexander Oskar</td>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113789</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Middle-aged male in first class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Mamee, Mr. Hanna</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2677</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>None</td>\n",
       "      <td>C</td>\n",
       "      <td>Male survivor in third class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Cann, Mr. Ernest Charles</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A./5. 2152</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Young male in third class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Miss. Augusta Maria</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>345764</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "      <td>Female in third class with family members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Nicola-Yarred, Miss. Jamila</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2651</td>\n",
       "      <td>11.2417</td>\n",
       "      <td>None</td>\n",
       "      <td>C</td>\n",
       "      <td>Female child in third class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Ahlin, Mrs. Johan (Johanna Persdotter Larsson)</td>\n",
       "      <td>female</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7546</td>\n",
       "      <td>9.4750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a female i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Turpin, Mrs. William John Robert (Dorothy Ann ...</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11668</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Low chance of survival due to being a female i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kraeff, Mr. Theodor</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349253</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>Low chance of survival due to being a male in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Laroche, Miss. Simonne Marie Anne Andree</td>\n",
       "      <td>female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>SC/Paris 2123</td>\n",
       "      <td>41.5792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>High chance of survival due to being a female ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Devaney, Miss. Margaret Delia</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330958</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>High chance of survival due to being a female ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
